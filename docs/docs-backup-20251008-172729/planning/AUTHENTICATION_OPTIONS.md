# Institutional Access Authentication - How It Works

## Current Implementation (Week 1-2)

### What's Working NOW

**‚úÖ No Authentication Required (30% coverage):**
- **PMC Full-Text**: Free, no login needed
- **Unpaywall API**: Free, no login needed
- Both work automatically with zero configuration

**üîó URL Generation (60% additional coverage):**
- **EZProxy URLs**: Generated but require manual browser login
- **OpenURL Resolvers**: Link to university library portals

### What Happens When You Search

```python
# When you run a search
result = pipeline.search("CRISPR cancer therapy")

# For each paper found
for paper in result.publications:
    # 1. Check PMC (free) - downloads automatically ‚úÖ
    # 2. Check Unpaywall (free) - downloads automatically ‚úÖ
    # 3. Generate Georgia Tech EZProxy URL - YOU need to click & login üîó
    # 4. Generate ODU EZProxy URL - YOU need to click & login üîó

    # Result metadata includes:
    paper.metadata['access_status'] = {
        'pmc': True/False,           # Auto-download ‚úÖ
        'unpaywall': True/False,      # Auto-download ‚úÖ
        'ezproxy': True/False,        # URL available, manual login needed üîó
        'openurl': True/False         # URL available, manual redirect üîó
    }

    paper.metadata['access_url'] = "https://login.ezproxy.gatech.edu/login?url=..."
    paper.metadata['access_instructions'] = "Click link, login with GT credentials"
```

## Authentication Options (Your Choice)

### **Option 1: Manual Browser Access (CURRENT - Recommended for Week 1-2)**

**How It Works:**
1. Pipeline finds papers and generates EZProxy URLs
2. You click the URL in results
3. Browser opens ‚Üí Georgia Tech login page
4. You login with your GT credentials
5. Browser redirects to full-text article
6. You can read/download manually

**Pros:**
- ‚úÖ **Zero setup** - works right now
- ‚úÖ **Most secure** - no credential storage
- ‚úÖ **Uses your existing browser login** - if already logged into GT portal, instant access
- ‚úÖ **No code changes needed**

**Cons:**
- ‚ùå Manual click required per paper
- ‚ùå Not automated

**Example Output:**
```python
result = pipeline.search("CRISPR cancer")

for paper in result.publications[:5]:
    print(f"Title: {paper.publication.title}")
    print(f"Access: {paper.metadata['access_url']}")
    print(f"Instructions: {paper.metadata['access_instructions']}")
    print("---")

# Output:
# Title: CRISPR-Cas9 gene editing for cancer therapy
# Access: https://login.ezproxy.gatech.edu/login?url=https://doi.org/10.1038/...
# Instructions: Open in browser, login with Georgia Tech credentials
# ---
```

---

### **Option 2: Automated with Stored Credentials (Week 4-5)**

**How It Works:**
1. You provide credentials ONCE (encrypted storage)
2. Pipeline uses Selenium/Playwright to automate browser
3. Logs in automatically
4. Downloads PDFs directly to disk
5. All papers accessible without manual intervention

**Pros:**
- ‚úÖ Fully automated
- ‚úÖ Batch download hundreds of papers
- ‚úÖ Integrated into pipeline

**Cons:**
- ‚ùå Requires credential storage (security risk if not done properly)
- ‚ùå Needs browser automation (Playwright/Selenium)
- ‚ùå More complex setup

**Implementation (NOT implemented yet):**
```python
# Future implementation (Week 4-5)
from omics_oracle_v2.lib.publications.config import PublicationSearchConfig

config = PublicationSearchConfig(
    enable_institutional_access=True,
    enable_automated_download=True,  # NEW - Week 4
    institutional_credentials={
        'gatech': {
            'username': os.getenv('GT_USERNAME'),
            'password': os.getenv('GT_PASSWORD')  # Encrypted
        }
    }
)

# Fully automated download
result = pipeline.search("CRISPR cancer", auto_download_pdfs=True)
# PDFs downloaded to data/pdfs/
```

**Security Requirements:**
```bash
# Environment variables (never commit!)
export GT_USERNAME="your_gt_username"
export GT_PASSWORD=$(echo "your_password" | openssl enc -aes-256-cbc -a -salt)

# Or use key vault
from azure.keyvault.secrets import SecretClient
password = vault_client.get_secret("gt-password").value
```

---

### **Option 3: Session Cookies (Week 3-4)**

**How It Works:**
1. You login ONCE manually in browser
2. Export cookies from browser
3. Pipeline reuses cookies for API requests
4. Auto-downloads PDFs while session valid

**Pros:**
- ‚úÖ No credential storage
- ‚úÖ Automated downloads
- ‚úÖ Uses your existing login

**Cons:**
- ‚ùå Sessions expire (re-login every few hours/days)
- ‚ùå Cookie extraction is manual
- ‚ùå May violate some publisher ToS

**Implementation (NOT implemented yet):**
```python
# Export cookies from Chrome
# 1. Login to Georgia Tech EZProxy
# 2. Use extension to export cookies.txt
# 3. Load cookies into pipeline

config = PublicationSearchConfig(
    enable_institutional_access=True,
    cookie_file="~/.omicsoracle/gt_cookies.txt"  # Week 3-4
)

# Downloads use your authenticated session
result = pipeline.search("CRISPR cancer")
```

---

### **Option 4: VPN/Proxy (Alternative)**

**How It Works:**
1. Connect to GT VPN
2. Your IP appears as Georgia Tech
3. Publisher sites grant automatic access
4. No EZProxy needed

**Pros:**
- ‚úÖ Transparent access
- ‚úÖ No URL rewriting
- ‚úÖ Works for all GT resources

**Cons:**
- ‚ùå Requires VPN connection
- ‚ùå All traffic routed through VPN
- ‚ùå May be slower

**Setup:**
```bash
# Connect to GT VPN first
sudo openvpn --config gt-vpn.ovpn

# Then run pipeline normally
# Publishers see GT IP, grant access automatically
```

---

## Recommended Approach (Phased)

### **Phase 1 (Week 1-2 - CURRENT)** ‚úÖ Implemented
**Method:** Manual browser access
**Coverage:** 30% free (PMC + Unpaywall) + 60% manual (EZProxy URLs)
**Setup:** Zero
**Good for:** Testing, validation, small-scale research

### **Phase 2 (Week 3)** üîÑ Next
**Method:** Add cookie-based session reuse
**Coverage:** 80-90% automated
**Setup:** Login once, export cookies
**Good for:** Active research, frequent use

### **Phase 3 (Week 4-5)** üìÖ Future
**Method:** Full automation with credential storage
**Coverage:** 80-90% fully automated
**Setup:** Secure credential configuration
**Good for:** Large-scale analysis, production deployment

### **Phase 4 (Week 6)** üìÖ Optional
**Method:** VPN integration + browser automation
**Coverage:** 95%+ automated
**Setup:** Complex but most comprehensive
**Good for:** Enterprise deployment

---

## Current Code Flow

```python
# What happens RIGHT NOW:

# 1. Search executes
result = pipeline.search("CRISPR cancer therapy")

# 2. For each paper found
for paper in result.publications:

    # Step 1: Try free access (auto-download) ‚úÖ
    if paper.pmcid:
        pdf = download_from_pmc(paper.pmcid)  # Works now!

    if not pdf and paper.doi:
        pdf = check_unpaywall(paper.doi)      # Works now!

    # Step 2: Generate institutional URLs (manual) üîó
    if not pdf:
        # Generate Georgia Tech EZProxy URL
        ezproxy_url = f"https://login.ezproxy.gatech.edu/login?url=https://doi.org/{paper.doi}"
        paper.metadata['access_url'] = ezproxy_url
        paper.metadata['access_instructions'] = (
            "1. Click URL\n"
            "2. Login with Georgia Tech credentials\n"
            "3. Article will open in browser\n"
            "4. Download PDF manually"
        )

    # Step 3: Store metadata
    paper.metadata['access_status'] = {
        'pmc': bool(paper.pmcid),
        'unpaywall': bool(unpaywall_found),
        'ezproxy': bool(ezproxy_url),
        'institutional_access_available': True
    }

# 3. Return results with access info
return PublicationResult(
    publications=sorted_papers,
    metadata={
        'auto_accessible': count_free,      # ~30%
        'manual_accessible': count_ezproxy,  # ~60%
        'total_accessible': count_all        # ~90%
    }
)
```

---

## What You Need to Do NOW

**Nothing!** The current implementation works for validation:

1. **Run a search:**
   ```python
   from omics_oracle_v2.lib.publications.pipeline import PublicationSearchPipeline
   from omics_oracle_v2.lib.publications.config import PublicationSearchConfig, PubMedConfig

   config = PublicationSearchConfig(
       enable_pubmed=True,
       enable_institutional_access=True,
       primary_institution="gatech",
       pubmed_config=PubMedConfig(email="your_email@gatech.edu")
   )

   pipeline = PublicationSearchPipeline(config)
   result = pipeline.search("CRISPR cancer therapy", max_results=10)
   ```

2. **Check results:**
   ```python
   for paper in result.publications[:5]:
       print(f"\nTitle: {paper.publication.title[:80]}...")
       print(f"Access Status: {paper.publication.metadata['access_status']}")

       if paper.publication.metadata.get('has_access'):
           url = paper.publication.metadata.get('access_url')
           print(f"Access URL: {url}")
           print(f"Instructions: Click URL, login with GT credentials")
   ```

3. **Click URLs in browser:**
   - Opens GT login page (if not already logged in)
   - Login with your credentials
   - Redirects to full article
   - Read/download manually

---

## Migration Path

When you're ready for automation (Week 3-4):

1. **Week 3: Add cookie support**
   ```python
   # Login once manually, export cookies
   config.enable_session_cookies = True
   config.cookie_file = "~/.omicsoracle/cookies.txt"
   ```

2. **Week 4: Add automated browser**
   ```python
   # Playwright automation
   config.enable_automated_download = True
   config.institutional_credentials = load_from_vault()
   ```

3. **Week 5: Production hardening**
   ```python
   # Full security, monitoring, rate limiting
   config.enable_secure_vault = True
   config.enable_download_monitoring = True
   ```

---

## Summary

**Current Status (Week 1-2):**
- ‚úÖ 30% auto-accessible (PMC + Unpaywall) - **working now**
- ‚úÖ 60% manual-accessible (EZProxy URLs) - **working now**
- ‚ùå Automated credential-based download - **Week 4**
- ‚ùå Browser automation - **Week 4-5**

**What You Get NOW:**
- Full metadata for 50+ papers per search
- Direct download for ~15 papers (30%)
- Access URLs for ~30 papers (60%)
- Click ‚Üí Login ‚Üí Read/Download

**What's Coming:**
- Week 3: Cookie-based automation (80% auto)
- Week 4: Credential automation (90% auto)
- Week 5: Production deployment (95%+ auto)

**Recommendation:**
Continue with current manual approach for Week 1-2 validation. It's secure, works immediately, and gives you access to 90% of papers with minimal effort.
