# Citation Scoring Decision Framework

**Purpose:** Help decide which citation scoring approach to implement  
**Audience:** Product/Engineering team  
**Decision Required By:** End of Week 4

---

## Decision Matrix

| Criterion | Current | +Velocity | +Semantic Scholar | Full ML |
|-----------|---------|-----------|-------------------|---------|
| **Accuracy** | 6/10 | 7.5/10 | 8.5/10 | 9.5/10 |
| **Implementation Time** | 0 hrs | 4-6 hrs | 1 week | 2 months |
| **Maintenance Burden** | Low | Low | Medium | High |
| **Data Dependencies** | None | None | API | API + Logs |
| **Explainability** | High | High | Medium | Low |
| **Cost** | $0 | $0 | $0 (free tier) | $$ (compute) |
| **Risk of Failure** | 0% | 5% | 20% | 60% |
| **User Value (immediate)** | Baseline | +20% | +40% | +60% |
| **User Value (6 months)** | Baseline | +15% | +50% | +80% |

**Legend:**
- Accuracy: Estimated % of queries where top-3 results match expert judgment
- Risk of Failure: Probability of implementation issues, API limitations, etc.
- User Value: Improvement in search relevance from user perspective

---

## Key Questions to Answer

### 1. What problem are we solving?

**User Complaint:** _"Old highly-cited papers dominate results, I can't find recent discoveries"_

**Evidence:**
- Week 2 analysis: HOMA-IR paper (2000, 30K citations) ranks high for "insulin resistance"
- User expectation: Recent papers (2023-2024) should rank higher for "latest" queries
- Current: Citations have 10% weight, Recency has 20% weight
- Issue: Absolute citation count doesn't account for time to accumulate

**Root Cause:**
- Citation count is **time-dependent**: older papers have more time to accumulate citations
- Our current scoring treats 30K citations (24 years) same as 500 citations (2 years)
- Both have different velocity: 1,250 cites/year vs 250 cites/year

**Proposed Solution:**
- Add citation velocity: citations per year
- Query intent detection: "recent" queries favor recency over citations

### 2. How critical is this problem?

**Priority Assessment:**

| Severity | Impact | Frequency | Priority |
|----------|--------|-----------|----------|
| Medium | Some users frustrated by old results | 30% of queries contain "recent" or year | **P1** |

**Why P1?**
- Affects core search quality (primary use case)
- Simple to fix (4-6 hours)
- High user impact (better results for 30% of queries)

**Why NOT P0?**
- Current system is functional (works for 70% of queries)
- Workaround exists (users can filter by date manually)
- No data loss risk

### 3. What are our constraints?

**Technical Constraints:**
- ‚úÖ Can calculate: citations, publication date, age (all in database)
- ‚ùå Cannot calculate easily: recent citation count (need API)
- ‚ùå Cannot calculate: influential citations (need API)
- ‚úÖ Can implement: Keyword-based intent detection (no ML needed)
- ‚ùå Cannot implement: Learning-to-rank (no user click data yet)

**Resource Constraints:**
- Time: Week 4 (ending soon), need to ship something
- Team: Small (1-2 engineers)
- Budget: $0 for external APIs (free tier only)

**Quality Constraints:**
- Must not break existing functionality
- Must be explainable to users
- Must handle edge cases (new papers with 0 citations)

### 4. What is the minimum viable solution?

**MVP Definition:**
Recent papers with moderate citation velocity should rank higher than old papers with high total citations (when query indicates recency intent).

**MVP Implementation:**
```python
# 1. Add citation velocity calculation
citations_per_year = total_citations / max(age_years, 0.1)

# 2. Combine absolute + velocity
citation_score = (absolute_score * 0.6) + (velocity_score * 0.4)

# 3. Query intent detection
if "recent" in query or "2024" in query:
    recency_weight = 0.40  # Increase from 0.20
    citation_weight = 0.05  # Decrease from 0.10
```

**MVP Acceptance Criteria:**
- [ ] Test case 1: Recent paper (2023, 100 cites, 50 cites/year) scores higher than old paper (2005, 1000 cites, 50 cites/year)
- [ ] Test case 2: Query "recent cancer discoveries" ranks 2024 papers in top 5
- [ ] Test case 3: Query "review of cancer" still ranks highly-cited reviews in top 5
- [ ] No regressions: Existing test suite passes
- [ ] Performance: Ranking latency <50ms (same as before)

**Time to MVP:** 4-6 hours

### 5. What are the alternatives?

**Alternative 1: Do Nothing**
- **Pros:** No effort, no risk
- **Cons:** User frustration continues, competitor advantage
- **Verdict:** ‚ùå Unacceptable (P1 issue)

**Alternative 2: Manual Date Filtering**
- **Pros:** User has full control
- **Cons:** Extra step, many users won't use it, poor UX
- **Verdict:** ‚ùå Band-aid solution

**Alternative 3: Simple Velocity (MVP)**
- **Pros:** Fast, low risk, addresses core issue
- **Cons:** Doesn't handle all edge cases, still somewhat naive
- **Verdict:** ‚úÖ **RECOMMENDED**

**Alternative 4: Semantic Scholar API**
- **Pros:** High quality metrics, handles edge cases
- **Cons:** API dependency, rate limits, 1 week implementation
- **Verdict:** ü§î Defer to Month 2 (after MVP validation)

**Alternative 5: Full ML Ranking**
- **Pros:** Optimal quality, adaptive
- **Cons:** Months of work, requires infrastructure, no user data yet
- **Verdict:** ‚ùå Premature optimization

---

## Risk Analysis

### Implementation Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Citation velocity calculation wrong | Low | High | Unit tests, manual validation |
| Query intent detection too simplistic | Medium | Medium | Start conservative, iterate |
| API rate limits (if using Semantic Scholar) | High | Medium | Implement caching, fallback |
| Performance regression | Low | High | Benchmark before/after |
| User confusion from ranking changes | Medium | Low | Document scoring, add explanations |

### Data Quality Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Missing publication dates | Low | Medium | Use current year as default |
| Citation counts outdated | Medium | Low | Accept staleness, refresh monthly |
| Papers with 0 citations penalized | High | Low | Set minimum score floor |
| Very new papers (<1 year) | High | Medium | Boost recency score for age <1 year |

### Product Risks

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Users prefer old highly-cited papers | Low | High | A/B test, allow toggle |
| Different users want different rankings | High | Medium | Intent detection, user preferences (future) |
| Ranking becomes unpredictable | Medium | High | Explainability, show score breakdown |

---

## Success Metrics

### How will we measure success?

**Quantitative Metrics:**
1. **Search Quality**
   - Manual evaluation: Do top-10 results match expert judgment?
   - Target: 80% match rate (up from current ~60%)

2. **User Satisfaction**
   - Click-through rate on top-3 results
   - Target: >50% (up from current ~35%)

3. **Performance**
   - Ranking latency: <50ms (no regression)
   - Cache hit rate: >90% (maintain current)

4. **Coverage**
   - % of queries where intent detected: Target >40%
   - % of publications with citation data: Target >95%

**Qualitative Metrics:**
1. User feedback: "Results feel more relevant"
2. Reduced complaints about old papers dominating
3. Positive mentions of recency handling

### Validation Plan

**Week 4 (This Week):**
- [ ] Implement MVP (citations per year + intent detection)
- [ ] Unit tests (edge cases)
- [ ] Manual testing (10 sample queries)
- [ ] Performance benchmark

**Month 2:**
- [ ] User survey (5-10 researchers)
- [ ] Expert evaluation (curated query set)
- [ ] Consider Semantic Scholar API integration

**Month 3:**
- [ ] A/B test (if traffic sufficient)
- [ ] Click data analysis
- [ ] Iterate based on feedback

---

## Decision Tree

```
START: Should we change citation scoring?

‚îú‚îÄ Is there a user problem?
‚îÇ  ‚îú‚îÄ No ‚Üí STOP (Don't fix what's not broken)
‚îÇ  ‚îî‚îÄ Yes ‚Üí Continue
‚îÇ
‚îú‚îÄ Is it a high-priority problem?
‚îÇ  ‚îú‚îÄ No ‚Üí Defer to backlog
‚îÇ  ‚îî‚îÄ Yes (P1) ‚Üí Continue
‚îÇ
‚îú‚îÄ Can we solve it in <1 week?
‚îÇ  ‚îú‚îÄ No ‚Üí Break into smaller phases
‚îÇ  ‚îî‚îÄ Yes ‚Üí Continue
‚îÇ
‚îú‚îÄ Do we have the data?
‚îÇ  ‚îú‚îÄ No ‚Üí Evaluate API options OR defer
‚îÇ  ‚îî‚îÄ Yes (citations + dates) ‚Üí Continue
‚îÇ
‚îú‚îÄ What's the simplest solution?
‚îÇ  ‚îú‚îÄ MVP: Citations per year ‚Üê YOU ARE HERE
‚îÇ  ‚îú‚îÄ Medium: API integration
‚îÇ  ‚îî‚îÄ Complex: ML ranking
‚îÇ
‚îî‚îÄ DECISION: Implement MVP (citations per year + intent detection)
   ‚îú‚îÄ Time: 4-6 hours
   ‚îú‚îÄ Risk: Low
   ‚îú‚îÄ Impact: Medium-High
   ‚îî‚îÄ Iterate: Month 2 (API), Month 6 (ML)
```

---

## Recommendation

### ‚úÖ Implement: Tier 1 (Citations Per Year + Intent Detection)

**Rationale:**
1. **User need is real:** 30% of queries contain recency indicators
2. **Simple solution exists:** Calculate citations/year (no API needed)
3. **Low risk:** Reversible, no external dependencies
4. **High impact:** Better results for 30% of queries
5. **Fast implementation:** 4-6 hours (fits in Week 4)

**Implementation Steps:**

```python
# Step 1: Add citations_per_year calculation (2 hours)
def _calculate_citation_velocity(citations, age_years):
    return citations / max(age_years, 0.1)

# Step 2: Combine absolute + velocity (1 hour)
citation_score = absolute * 0.6 + velocity * 0.4

# Step 3: Query intent detection (2 hours)
def detect_query_intent(query):
    if 'recent' in query or any(year in query for year in ['2023','2024','2025']):
        return 'recent'
    return 'balanced'

# Step 4: Tests + validation (2 hours)
- Unit tests for edge cases
- Manual testing with 10 queries
- Performance benchmark
```

**Total Effort:** 6-8 hours (including testing)

### ü§î Defer: Tier 2 (Semantic Scholar API)

**Rationale:**
1. **Need to validate Tier 1 first:** Ensure simple approach works
2. **API constraints unknown:** Rate limits, coverage, reliability
3. **More complex:** 1 week implementation + maintenance
4. **Diminishing returns:** 70% benefit from Tier 1, +20% from Tier 2

**When to revisit:** Month 2 (after Tier 1 validation)

### ‚ùå Don't Implement: Tier 3 (ML Ranking)

**Rationale:**
1. **No user data yet:** Can't train model without click logs
2. **Over-engineering:** Tier 1 + Tier 2 likely sufficient
3. **High cost:** Months of work, infrastructure, maintenance
4. **Premature:** Need user feedback on simpler approaches first

**When to revisit:** Month 6+ (if user base grows significantly)

---

## Next Steps

### This Week (Week 4)

**Day 1 (Today):**
- [x] Research citation scoring methods (DONE - this document)
- [ ] Review research with team
- [ ] Decide: Implement Tier 1 or defer entirely

**Day 2:**
- [ ] Implement citations_per_year calculation
- [ ] Implement query intent detection
- [ ] Unit tests for edge cases

**Day 3:**
- [ ] Manual validation (10 sample queries)
- [ ] Performance benchmarking
- [ ] Bug fixes if needed

**Day 4:**
- [ ] Code review
- [ ] Documentation updates
- [ ] Commit + tag as week4-day1-complete

**Day 5:**
- [ ] User testing (if possible)
- [ ] Week 4 retrospective

### Month 2

- [ ] Collect user feedback on Tier 1 changes
- [ ] Evaluate Semantic Scholar API
- [ ] Decide: Implement Tier 2 or stay with Tier 1

### Month 3+

- [ ] A/B testing framework
- [ ] Click data collection
- [ ] Iterative improvements based on data

---

## Approval Checklist

Before implementing, confirm:

- [ ] **Problem validated:** Users actually want this (not just our assumption)
- [ ] **Data available:** We have citations + dates for >95% of papers
- [ ] **Tests defined:** We know how to validate success
- [ ] **Rollback plan:** Can revert to current scoring if needed
- [ ] **Documentation:** Users understand how scoring works
- [ ] **Team alignment:** Everyone agrees on approach

---

## Appendix: Sample Queries for Validation

### Test Set 1: Recency Intent

| Query | Expected Behavior |
|-------|-------------------|
| "recent breast cancer discoveries" | 2024 papers in top 5 |
| "latest CRISPR applications 2024" | 2023-2024 papers dominate |
| "new Alzheimer's treatments" | 2022+ papers rank high |

### Test Set 2: Review Intent

| Query | Expected Behavior |
|-------|-------------------|
| "review of cancer genomics" | Highly-cited reviews (any year) |
| "overview of RNA-seq methods" | Foundational methods papers |
| "meta-analysis diabetes" | Meta-analysis papers preferred |

### Test Set 3: Balanced Intent

| Query | Expected Behavior |
|-------|-------------------|
| "APOE Alzheimer's disease" | Mix of classics + recent |
| "insulin resistance diabetes" | HOMA-IR paper + recent studies |
| "machine learning genomics" | Mix of foundational + recent |

### Test Set 4: Dataset Intent

| Query | Expected Behavior |
|-------|-------------------|
| "breast cancer GEO datasets" | Citation score barely matters |
| "GSE12345" | Exact dataset match |
| "RNA-seq data Alzheimer's" | Datasets ranked by metadata match |

---

**Document Status:** FINAL - Ready for team review  
**Estimated Review Time:** 20-30 minutes  
**Decision Required:** Go/No-Go on Tier 1 implementation

