# Data Flow and Mapping Mechanism: Download â†’ AI Analysis

**Date:** October 13, 2025
**Purpose:** Explain how OmicsOracle maintains correct dataset-to-PDF mappings when downloading multiple datasets and analyzing them in different order

---

## Question

> "When we click download on multiple cards on the frontend and then click on AI Analysis button after some time, not in the same order as downloaded, how does it manage to get correct parsed text instead of mixing up with other dataset parsed text? How does that mapping mechanism work?"

---

## Answer: The Unique Identifier Chain

The system maintains **dataset identity through a chain of unique identifiers** that flows from frontend â†’ backend â†’ database â†’ AI analysis. Here's the complete mechanism:

### ğŸ”‘ Key Identifier: `geo_id`

Every dataset has a **unique GEO ID** (e.g., `GSE123456`) that serves as the primary key throughout the entire system. This ID:
- Originates from NCBI GEO database (globally unique)
- Stays with the dataset object through all operations
- Links datasets to their downloaded PDFs
- Ensures correct PDF-to-dataset mapping in AI analysis

---

## Complete Data Flow (Step-by-Step)

### ğŸ“Š **Phase 1: Search Results Display**

**Frontend (`dashboard_v2.html`):**
```javascript
// Search returns array of datasets with unique IDs
currentResults = [
    {
        geo_id: "GSE123456",        // â† UNIQUE IDENTIFIER
        title: "Cancer Study A",
        pubmed_ids: ["12345", "67890"],
        fulltext: [],               // Empty initially
        fulltext_count: 0,
        fulltext_status: "not_downloaded"
    },
    {
        geo_id: "GSE789012",        // â† DIFFERENT UNIQUE ID
        title: "Cancer Study B",
        pubmed_ids: ["11111", "22222"],
        fulltext: [],
        fulltext_count: 0,
        fulltext_status: "not_downloaded"
    }
]

// Render cards with array index for DOM access
displayResults(currentResults);
```

**Each card rendered with its array index:**
```html
<div class="dataset-card" id="card-0">    <!-- Index 0 â†’ GSE123456 -->
    <button onclick="downloadPapersForDataset(0)">Download Papers</button>
    <button onclick="selectDataset(0)">AI Analysis</button>
</div>

<div class="dataset-card" id="card-1">    <!-- Index 1 â†’ GSE789012 -->
    <button onclick="downloadPapersForDataset(1)">Download Papers</button>
    <button onclick="selectDataset(1)">AI Analysis</button>
</div>
```

---

### ğŸ“¥ **Phase 2: Download Papers (User Clicks Card 1, Then Card 0)**

**Click 1: Download GSE789012 (index 1)**

```javascript
async function downloadPapersForDataset(index) {
    // Get dataset from currentResults using index
    const dataset = currentResults[index];  // GSE789012

    // Send ENTIRE dataset object to backend
    const response = await fetch('/api/agents/enrich-fulltext', {
        method: 'POST',
        body: JSON.stringify([dataset])  // â† Includes geo_id: "GSE789012"
    });

    const enrichedDatasets = await response.json();
    const enriched = enrichedDatasets[0];  // Backend returns enriched copy

    // âœ… UPDATE THE SAME INDEX with enriched data
    currentResults[index] = enriched;  // Replace GSE789012 with enriched version

    // Re-render ALL cards (maintains order)
    displayResults(currentResults);
}
```

**Backend Processing (`agents.py` - `/enrich-fulltext` endpoint):**

```python
@router.post("/enrich-fulltext")
async def enrich_fulltext(
    datasets: List[DatasetResponse],  # [dataset with geo_id="GSE789012"]
    ...
):
    enriched_datasets = []

    for dataset in datasets:  # Process GSE789012
        # âœ… Dataset object CARRIES its geo_id throughout
        logger.info(f"Enriching {dataset.geo_id}...")  # "GSE789012"

        # STEP 1: Get PMIDs from dataset object
        pmids_to_fetch = dataset.pubmed_ids  # ["11111", "22222"]

        # STEP 2: Download PDFs for these PMIDs
        publications = []
        for pmid in pmids_to_fetch:
            pub = pubmed_client.fetch_by_id(pmid)
            publications.append(pub)

        # STEP 3: Download PDFs (saved with PMID in filename)
        pdf_dir = Path(f"data/fulltext/pdfs")
        download_report = await pdf_downloader.download_batch(
            publications=publications,
            output_dir=pdf_dir
        )

        # PDFs saved as: data/fulltext/pdfs/PMID_11111.pdf
        #                data/fulltext/pdfs/PMID_22222.pdf

        # STEP 4: Parse PDFs and ATTACH to dataset.fulltext array
        dataset.fulltext = []  # â† Attached to THIS dataset object

        for pub in publications:
            if hasattr(pub, "pdf_path") and pub.pdf_path:
                # Parse the PDF
                parsed_content = await fulltext_manager.get_parsed_content(pub)

                # âœ… Add parsed text to THIS dataset's fulltext array
                dataset.fulltext.append({
                    "pmid": pub.pmid,         # "11111"
                    "title": pub.title,
                    "abstract": parsed_content.get("abstract", ""),
                    "methods": parsed_content.get("methods", ""),
                    "results": parsed_content.get("results", ""),
                    "discussion": parsed_content.get("discussion", ""),
                    "pdf_path": str(pub.pdf_path)  # Link to actual PDF file
                })

        # Update metadata
        dataset.fulltext_count = len(dataset.fulltext)  # 2 papers
        dataset.fulltext_status = "available"

        # âœ… Return enriched dataset WITH SAME geo_id
        enriched_datasets.append(dataset)  # Still geo_id="GSE789012"

    return enriched_datasets
```

**Click 2: Download GSE123456 (index 0) - Same Process**

The second download operates **independently** on a different dataset object:
- Uses `currentResults[0]` which has `geo_id="GSE123456"`
- Downloads different PMIDs: `["12345", "67890"]`
- Parses different PDFs: `PMID_12345.pdf`, `PMID_67890.pdf`
- Attaches parsed text to `dataset.fulltext` array of **GSE123456 object**
- Updates `currentResults[0]` with enriched GSE123456

**Result After Both Downloads:**

```javascript
currentResults = [
    {
        geo_id: "GSE123456",           // â† Still unique identifier
        fulltext: [
            { pmid: "12345", methods: "Methods from paper 12345...", ... },
            { pmid: "67890", methods: "Methods from paper 67890...", ... }
        ],
        fulltext_count: 2,
        fulltext_status: "available"
    },
    {
        geo_id: "GSE789012",           // â† Different unique identifier
        fulltext: [
            { pmid: "11111", methods: "Methods from paper 11111...", ... },
            { pmid: "22222", methods: "Methods from paper 22222...", ... }
        ],
        fulltext_count: 2,
        fulltext_status: "available"
    }
]
```

**âœ… No mixing! Each dataset's `fulltext` array contains ONLY its own papers.**

---

### ğŸ¤– **Phase 3: AI Analysis (User Clicks Card 0, Then Card 1)**

**Click 1: Analyze GSE123456 (index 0)**

```javascript
async function selectDataset(index) {
    // Get dataset from currentResults using index
    selectedDataset = currentResults[index];  // GSE123456 with its fulltext
    await analyzeDatasetInline(selectedDataset, index);
}

async function analyzeDatasetInline(dataset, index) {
    // Send ENTIRE dataset object (includes geo_id + fulltext array)
    const response = await fetch('/api/agents/analyze', {
        method: 'POST',
        body: JSON.stringify({
            datasets: [dataset],  // â† Includes geo_id: "GSE123456" + its fulltext
            query: currentQuery,
            max_datasets: 1
        })
    });

    const analysis = await response.json();
    displayAnalysisInline(analysis, dataset, contentElement);
}
```

**Backend AI Analysis (`agents.py` - `/analyze` endpoint):**

```python
@router.post("/analyze")
async def analyze_datasets(request: AIAnalysisRequest):
    datasets_to_analyze = request.datasets  # [dataset with geo_id="GSE123456"]

    # Check full-text availability
    total_fulltext_count = sum(
        len(ds.fulltext) if ds.fulltext else 0
        for ds in datasets_to_analyze
    )  # Count: 2 papers from GSE123456

    # Build analysis prompt
    dataset_summaries = []

    for i, ds in enumerate(datasets_to_analyze, 1):
        dataset_info = [
            f"{i}. **{ds.geo_id}** (Relevance: {int(ds.relevance_score * 100)}%)",
            f"   Title: {ds.title}",
            f"   GEO Summary: {ds.summary[:200]}...",
        ]

        # âœ… Iterate through THIS dataset's fulltext array
        if ds.fulltext and len(ds.fulltext) > 0:
            dataset_info.append(
                f"\n   Full-text content from {len(ds.fulltext)} publication(s):"
            )

            # âœ… Use parsed text from THIS dataset's papers
            for j, ft in enumerate(ds.fulltext[:2], 1):
                dataset_info.extend([
                    f"\n   Paper {j}: {ft.title[:100]}... (PMID: {ft.pmid})",
                    f"   Abstract: {ft.abstract[:250]}...",
                    f"   Methods: {ft.methods[:400]}...",      # â† From PMID 12345
                    f"   Results: {ft.results[:400]}...",      # â† From PMID 12345
                    f"   Discussion: {ft.discussion[:250]}...",
                ])

        dataset_summaries.append("\n".join(dataset_info))

    # Send to GPT-4
    prompt = f"""
    User searched for: "{request.query}"

    Found {len(datasets_to_analyze)} relevant datasets:

    {chr(10).join(dataset_summaries)}

    Analyze these datasets and provide insights...
    """

    analysis = await ai_client.summarize_search(prompt)
    return AIAnalysisResponse(analysis=analysis, ...)
```

**Click 2: Analyze GSE789012 (index 1) - Independent Operation**

Same process but with **different dataset object**:
- Uses `currentResults[1]` with `geo_id="GSE789012"`
- Accesses `dataset.fulltext` containing papers 11111 and 22222
- Builds prompt with Methods/Results from **different PDFs**
- Sends to GPT-4 for **independent analysis**

---

## ğŸ”’ Why Mixing Cannot Occur

### 1. **Object Encapsulation**
```javascript
// Each dataset is a SEPARATE JavaScript object
const dataset1 = {
    geo_id: "GSE123456",
    fulltext: [/* papers for GSE123456 */]
};

const dataset2 = {
    geo_id: "GSE789012",
    fulltext: [/* papers for GSE789012 */]
};

// They NEVER share references
dataset1.fulltext !== dataset2.fulltext  // âœ… Different arrays
```

### 2. **Backend Loops Over Input**
```python
for dataset in datasets:  # Process each dataset independently
    dataset.fulltext = []  # NEW array for THIS dataset
    for pub in publications:  # Publications fetched using THIS dataset's PMIDs
        dataset.fulltext.append(...)  # Append to THIS dataset's array
```

### 3. **Array Index Stability**
```javascript
currentResults[0] = enrichedGSE123456;  // Always index 0
currentResults[1] = enrichedGSE789012;  // Always index 1

// When user clicks button, correct index is used
button.onclick = () => selectDataset(0);  // Always gets GSE123456
```

### 4. **Database File Isolation**
```bash
# PDFs are saved with PMID-based filenames (globally unique)
data/fulltext/pdfs/
â”œâ”€â”€ PMID_12345.pdf   # From GSE123456
â”œâ”€â”€ PMID_67890.pdf   # From GSE123456
â”œâ”€â”€ PMID_11111.pdf   # From GSE789012
â””â”€â”€ PMID_22222.pdf   # From GSE789012

# Each dataset's fulltext array references its own PDF files
GSE123456.fulltext[0].pdf_path = "data/fulltext/pdfs/PMID_12345.pdf"
GSE789012.fulltext[0].pdf_path = "data/fulltext/pdfs/PMID_11111.pdf"
```

---

## ğŸ”„ Complete Loop Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND: Click "Download Papers" on Card 1 (GSE789012)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ 1. Get dataset object: currentResults[1]
             â”‚    â†’ { geo_id: "GSE789012", pubmed_ids: ["11111", "22222"] }
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API CALL: POST /api/agents/enrich-fulltext                     â”‚
â”‚           Body: [{ geo_id: "GSE789012", pubmed_ids: [...] }]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKEND: enrich_fulltext() Function                            â”‚
â”‚                                                                 â”‚
â”‚ for dataset in datasets:  # dataset.geo_id = "GSE789012"       â”‚
â”‚     â”œâ”€ Fetch publications for PMIDs [11111, 22222]             â”‚
â”‚     â”œâ”€ Download PDFs â†’ PMID_11111.pdf, PMID_22222.pdf          â”‚
â”‚     â”œâ”€ Parse PDFs â†’ extract Methods, Results, Discussion       â”‚
â”‚     â””â”€ Attach to dataset.fulltext = [                          â”‚
â”‚           { pmid: "11111", methods: "...", results: "..." },    â”‚
â”‚           { pmid: "22222", methods: "...", results: "..." }     â”‚
â”‚        ]                                                        â”‚
â”‚                                                                 â”‚
â”‚ Return: [{ geo_id: "GSE789012", fulltext: [...], ... }]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND: Receive enriched dataset                             â”‚
â”‚           currentResults[1] = enrichedDataset                   â”‚
â”‚           displayResults(currentResults)  # Re-render all cards â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

             ... (time passes, user downloads other datasets) ...

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND: Click "AI Analysis" on Card 1 (GSE789012)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ 1. Get dataset object: currentResults[1]
             â”‚    â†’ { geo_id: "GSE789012", fulltext: [11111, 22222] }
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API CALL: POST /api/agents/analyze                             â”‚
â”‚           Body: { datasets: [{ geo_id: "GSE789012", ... }] }    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BACKEND: analyze_datasets() Function                           â”‚
â”‚                                                                 â”‚
â”‚ for ds in datasets_to_analyze:  # ds.geo_id = "GSE789012"      â”‚
â”‚     if ds.fulltext and len(ds.fulltext) > 0:                   â”‚
â”‚         for ft in ds.fulltext:  # ft.pmid in ["11111", "22222"]â”‚
â”‚             prompt += f"Methods: {ft.methods[:400]}..."         â”‚
â”‚             prompt += f"Results: {ft.results[:400]}..."         â”‚
â”‚                                                                 â”‚
â”‚ Send prompt to GPT-4 with Methods/Results from papers 11111    â”‚
â”‚ and 22222 (NOT papers 12345/67890 from GSE123456!)             â”‚
â”‚                                                                 â”‚
â”‚ Return: AIAnalysisResponse with analysis text                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND: Display analysis inline in Card 1                    â”‚
â”‚           analysisContent.innerHTML = analysis.analysis         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Takeaways

| **Component** | **Mechanism** | **Why It Works** |
|---------------|---------------|------------------|
| **Unique ID** | `geo_id` (e.g., "GSE123456") | Globally unique from NCBI GEO database |
| **Data Structure** | `dataset.fulltext = []` array | Each dataset has its own array instance |
| **File Storage** | `PMID_12345.pdf` | PMIDs are globally unique identifiers |
| **Array Index** | `currentResults[index]` | Stable index-to-dataset mapping |
| **Backend Loop** | `for dataset in datasets:` | Processes each dataset independently |
| **Object Passing** | Full dataset object sent | geo_id + fulltext travel together |

### Why Order Doesn't Matter

```javascript
// Download order: Card 1, Card 0, Card 2
downloadPapersForDataset(1);  // Updates currentResults[1]
downloadPapersForDataset(0);  // Updates currentResults[0]
downloadPapersForDataset(2);  // Updates currentResults[2]

// AI Analysis order: Card 0, Card 2, Card 1
selectDataset(0);  // Uses currentResults[0] (correct!)
selectDataset(2);  // Uses currentResults[2] (correct!)
selectDataset(1);  // Uses currentResults[1] (correct!)

// âœ… Each operation uses the correct array index
// âœ… Each dataset object maintains its own fulltext array
// âœ… No shared state between dataset objects
```

---

## ğŸ” Verification

You can verify this mechanism by:

1. **Check Browser Console:**
   ```javascript
   // After downloading papers for multiple datasets
   console.log(currentResults[0].geo_id);  // "GSE123456"
   console.log(currentResults[0].fulltext);  // Papers for GSE123456

   console.log(currentResults[1].geo_id);  // "GSE789012"
   console.log(currentResults[1].fulltext);  // Papers for GSE789012 (different!)
   ```

2. **Check Backend Logs:**
   ```
   [INFO] Enriching GSE789012 with 2 PMIDs...
   [OK] PMID 11111: pdf_path=PMID_11111.pdf
   [OK] PMID 22222: pdf_path=PMID_22222.pdf
   [DATA] Added 2 entries to fulltext

   [INFO] Analyzing GSE789012...
   [DOC] PMID 11111: methods_len=1234 chars
   [DOC] PMID 22222: methods_len=2345 chars
   ```

3. **Check PDF Files:**
   ```bash
   ls data/fulltext/pdfs/
   # PMID_11111.pdf  â† Belongs to GSE789012
   # PMID_22222.pdf  â† Belongs to GSE789012
   # PMID_12345.pdf  â† Belongs to GSE123456
   # PMID_67890.pdf  â† Belongs to GSE123456
   ```

---

## ğŸ›¡ï¸ Safety Guarantees

1. **Immutable IDs:** `geo_id` never changes throughout the pipeline
2. **Isolated Arrays:** Each dataset has its own `fulltext` array instance
3. **Unique Filenames:** PMIDs are globally unique (no collisions)
4. **Synchronous Updates:** `currentResults[index]` replaced atomically
5. **Backend Validation:** Backend processes one dataset at a time in loops
6. **No Global State:** No shared mutable state between dataset objects

---

## ğŸ“ Summary

**The mapping mechanism relies on three levels of isolation:**

1. **Frontend:** Array index (`currentResults[index]`) maps button clicks to specific dataset objects
2. **Backend:** Loop iteration (`for dataset in datasets:`) processes each dataset independently
3. **Database:** PMID-based filenames (`PMID_12345.pdf`) ensure no file collisions

**When you click AI Analysis:**
- Frontend gets dataset from `currentResults[index]` âœ…
- Dataset object contains `geo_id` + its own `fulltext` array âœ…
- Backend iterates through `dataset.fulltext` (only that dataset's papers) âœ…
- GPT-4 receives Methods/Results from correct PDFs âœ…

**Result: Perfect dataset-to-PDF mapping, regardless of download/analysis order!** ğŸ¯

---

**Related Files:**
- Frontend: `omics_oracle_v2/api/static/dashboard_v2.html` (lines 1200-1250, 1502-1602)
- Backend: `omics_oracle_v2/api/routes/agents.py` (lines 390-680, 750-880)
- Models: `omics_oracle_v2/api/models/responses.py` (lines 69-149)
