# Storage Structure Evaluation: Source-Based vs Alternatives

**Date:** October 11, 2025
**Question:** Is source-based directory structure the best approach for our use case?

---

## Current Proposed Structure (Source-Based)

```
data/fulltext/
â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ arxiv/          # arXiv papers
â”‚   â”œâ”€â”€ pmc/            # PubMed Central PDFs
â”‚   â”œâ”€â”€ institutional/  # Georgia Tech/ODU downloads
â”‚   â”œâ”€â”€ publisher/      # Direct from publisher
â”‚   â”œâ”€â”€ scihub/         # Sci-Hub downloads
â”‚   â””â”€â”€ {hash}.pdf      # Legacy cache
â”œâ”€â”€ xml/
â”‚   â””â”€â”€ pmc/            # PMC NXML files
â””â”€â”€ parsed/
    â””â”€â”€ {id}.json       # Parsed content cache
```

### Pros âœ…
1. **Clear provenance** - Know exactly where each file came from
2. **Legal separation** - Easy to delete scihub/ if needed for compliance
3. **Source-specific optimization** - Can apply different parsing strategies per source
4. **Debugging** - "Show me all institutional downloads" is trivial
5. **Quality tracking** - Monitor success rates by source
6. **Selective re-parsing** - "Re-parse only arXiv papers" is simple

### Cons âŒ
1. **Duplicate file risk** - Same paper from multiple sources = multiple files
2. **More complex lookups** - Must check 7+ directories
3. **Identifier dependency** - Need to know source to find file efficiently
4. **Migration complexity** - Moving between sources requires file moves

---

## Alternative 1: Flat Structure (All Files Together)

```
data/fulltext/
â”œâ”€â”€ pdf/
â”‚   â””â”€â”€ {identifier}.pdf    # All PDFs together
â”œâ”€â”€ xml/
â”‚   â””â”€â”€ {identifier}.xml    # All XMLs together
â””â”€â”€ parsed/
    â””â”€â”€ {identifier}.json   # Parsed cache
```

### Implementation
```python
# Single lookup
pdf_file = Path(f"data/fulltext/pdf/{publication.pmc_id}.pdf")
if pdf_file.exists():
    return pdf_file
```

### Pros âœ…
1. **Simple lookups** - Single directory check
2. **Fast filesystem** - No subdirectory traversal
3. **Deduplication** - Same identifier = same file (automatic dedup)
4. **Easy backup** - `rsync data/fulltext/pdf/` backs up everything
5. **No duplicate storage** - Paper downloaded from multiple sources only stored once

### Cons âŒ
1. **Lost provenance** - Can't tell source without metadata DB
2. **No legal separation** - Can't easily delete Sci-Hub files
3. **Huge directories** - Millions of files in one folder (filesystem limits)
4. **Lost debugging** - Can't analyze by source
5. **Identifier conflicts** - What if PMC ID = arXiv ID?

### Filesystem Limits
```
ext4: 10 million files per directory (slow after 100k)
APFS: No hard limit, but slow after ~500k files
XFS: Better, but still slow after 1M files
```

**Verdict:** âŒ **NOT RECOMMENDED** - Will hit filesystem limits at scale

---

## Alternative 2: Content-Addressable (Hash-Based)

```
data/fulltext/
â”œâ”€â”€ pdf/
â”‚   â””â”€â”€ ab/
â”‚       â””â”€â”€ cd/
â”‚           â””â”€â”€ abcd1234efgh5678.pdf  # Content hash
â”œâ”€â”€ xml/
â”‚   â””â”€â”€ 12/
â”‚       â””â”€â”€ 34/
â”‚           â””â”€â”€ 12345678abcd.xml
â””â”€â”€ metadata/
    â””â”€â”€ {publication_id}.json  # Maps ID â†’ hash + source
```

### Implementation (Git-like)
```python
import hashlib

# Store
content_hash = hashlib.sha256(pdf_content).hexdigest()
prefix = content_hash[:2]
subdir = content_hash[2:4]
path = Path(f"data/fulltext/pdf/{prefix}/{subdir}/{content_hash}.pdf")

# Metadata
metadata = {
    'publication_id': pub.id,
    'content_hash': content_hash,
    'source': 'arxiv',
    'doi': pub.doi,
    'downloaded_at': '2025-10-11T12:00:00Z'
}
```

### Pros âœ…
1. **Automatic deduplication** - Identical content = same file (saves space)
2. **Integrity verification** - Filename IS the checksum
3. **Scales infinitely** - 256 * 256 = 65,536 subdirectories
4. **Fast lookups** - O(1) with hash, no directory scanning
5. **Corruption detection** - Re-hash file to verify integrity
6. **Publisher update detection** - Updated PDF = different hash

### Cons âŒ
1. **Extra metadata layer** - MUST maintain publication_id â†’ hash mapping
2. **Lost human readability** - Can't eyeball "what's this file?"
3. **Metadata dependency** - If metadata DB corrupted, files are orphaned
4. **Debugging harder** - "Show me arXiv papers" requires metadata query
5. **Overhead** - Must hash 100MB PDF before storing

### Real-World Example (Git)
```
.git/objects/
â”œâ”€â”€ ab/
â”‚   â””â”€â”€ cdef1234...  # Git uses this, works great
â””â”€â”€ cd/
    â””â”€â”€ 5678abcd...

# Git also keeps metadata (refs, logs, etc.)
```

**Verdict:** âš ï¸ **GOOD FOR LARGE SCALE** - But requires robust metadata DB

---

## Alternative 3: Hybrid (Source + Identifier)

```
data/fulltext/
â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ arxiv/
â”‚   â”‚   â””â”€â”€ 23/          # Year-based
â”‚   â”‚       â””â”€â”€ 01/      # Month-based
â”‚   â”‚           â””â”€â”€ 2301.12345.pdf
â”‚   â”œâ”€â”€ pmc/
â”‚   â”‚   â””â”€â”€ 98/          # First 2 digits of PMC ID
â”‚   â”‚       â””â”€â”€ 76/      # Next 2 digits
â”‚   â”‚           â””â”€â”€ PMC9876543.pdf
â”‚   â””â”€â”€ institutional/
â”‚       â””â”€â”€ 2025/        # Year downloaded
â”‚           â””â”€â”€ 10/      # Month downloaded
â”‚               â””â”€â”€ {sanitized_doi}.pdf
â”œâ”€â”€ xml/
â”‚   â””â”€â”€ pmc/
â”‚       â””â”€â”€ 98/
â”‚           â””â”€â”€ 76/
â”‚               â””â”€â”€ PMC9876543.nxml
â”œâ”€â”€ parsed/
â”‚   â””â”€â”€ {first_2_chars}/
â”‚       â””â”€â”€ {next_2_chars}/
â”‚           â””â”€â”€ {publication_id}.json
â””â”€â”€ metadata.db          # SQLite: pub_id â†’ file_path + source + metadata
```

### Pros âœ…
1. **Provenance preserved** - Source in path
2. **Scales well** - Subdirectories keep each folder manageable
3. **Human readable** - Can still browse by source/date
4. **Legal separation** - Delete scihub/ still works
5. **Fast lookups** - Predictable paths, no full directory scan
6. **Time-based analysis** - "Show me papers from 2024"

### Cons âŒ
1. **More complex paths** - Deeper nesting
2. **Still potential duplicates** - Same paper from multiple sources
3. **Path calculation** - Must extract year/month/prefix from identifiers

**Verdict:** âœ… **BEST BALANCE** - Combines benefits of source-based + scalability

---

## Alternative 4: Database-Centric (Minimal Files)

```
data/fulltext/
â”œâ”€â”€ blobs/
â”‚   â””â”€â”€ {hash}.blob      # Just raw files, no structure
â””â”€â”€ database/
    â””â”€â”€ fulltext.db      # All metadata + paths

# Database schema
CREATE TABLE fulltext_files (
    publication_id TEXT PRIMARY KEY,
    file_hash TEXT,
    file_type TEXT,
    source TEXT,
    file_path TEXT,
    size_bytes INTEGER,
    downloaded_at TIMESTAMP,
    quality_score REAL,
    INDEX idx_source (source),
    INDEX idx_hash (file_hash)
);
```

### Pros âœ…
1. **Single source of truth** - Database knows everything
2. **Powerful queries** - "Find all PDFs from institutional access in 2024"
3. **Easy deduplication** - Query by hash before storing
4. **Flexible metadata** - Add columns without restructuring files
5. **Analytics ready** - Built-in reporting

### Cons âŒ
1. **Database dependency** - If DB corrupted, everything lost
2. **Backup complexity** - Must backup DB + files together
3. **Opaque storage** - Can't browse files without DB
4. **Migration risk** - DB schema changes require migrations

**Verdict:** âœ… **BEST FOR PRODUCTION** - But needs good backup strategy

---

## Recommended Hybrid Approach

### Final Recommendation: **Hybrid Source-Based + Database**

```
data/fulltext/
â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ arxiv/
â”‚   â”‚   â””â”€â”€ {arxiv_id}.pdf         # E.g., 2301.12345.pdf
â”‚   â”œâ”€â”€ pmc/
â”‚   â”‚   â””â”€â”€ PMC{id}.pdf            # E.g., PMC9876543.pdf
â”‚   â”œâ”€â”€ institutional/
â”‚   â”‚   â””â”€â”€ {sanitized_doi}.pdf    # E.g., 10_1234_test_2023_001.pdf
â”‚   â”œâ”€â”€ publisher/
â”‚   â”‚   â””â”€â”€ {sanitized_doi}.pdf
â”‚   â”œâ”€â”€ scihub/
â”‚   â”‚   â””â”€â”€ {sanitized_doi}.pdf
â”‚   â””â”€â”€ biorxiv/
â”‚       â””â”€â”€ {doi_suffix}.pdf
â”œâ”€â”€ xml/
â”‚   â””â”€â”€ pmc/
â”‚       â””â”€â”€ PMC{id}.nxml
â”œâ”€â”€ parsed/
â”‚   â””â”€â”€ {publication_id}.json      # Fast access cache
â””â”€â”€ metadata/
    â”œâ”€â”€ fulltext.db                # SQLite metadata
    â””â”€â”€ checksums.txt              # Backup verification
```

### Database Schema
```sql
CREATE TABLE fulltext_cache (
    publication_id TEXT PRIMARY KEY,
    doi TEXT,
    pmid TEXT,
    pmc_id TEXT,

    -- File info
    file_path TEXT NOT NULL,
    file_type TEXT NOT NULL,      -- 'pdf', 'xml', 'nxml'
    file_source TEXT NOT NULL,    -- 'arxiv', 'pmc', 'institutional', etc.
    file_hash TEXT,               -- SHA256 for integrity
    file_size_bytes INTEGER,

    -- Timestamps
    downloaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    parsed_at TIMESTAMP,
    last_accessed TIMESTAMP,

    -- Content metadata
    has_fulltext BOOLEAN DEFAULT TRUE,
    has_tables BOOLEAN DEFAULT FALSE,
    table_count INTEGER DEFAULT 0,
    figure_count INTEGER DEFAULT 0,
    word_count INTEGER,
    quality_score REAL,

    -- Indexing
    INDEX idx_doi (doi),
    INDEX idx_pmc_id (pmc_id),
    INDEX idx_source (file_source),
    INDEX idx_has_tables (has_tables),
    INDEX idx_downloaded (downloaded_at)
);

-- Deduplication tracking
CREATE TABLE file_duplicates (
    file_hash TEXT,
    publication_id TEXT,
    file_path TEXT,
    PRIMARY KEY (file_hash, publication_id)
);
```

### Implementation
```python
class SmartCacheWithDB:
    """Enhanced SmartCache with database metadata."""

    def find_local_file(self, publication):
        """Check DB first, then filesystem."""

        # FAST PATH: Check database
        cached = self.db.get_cached_file(publication.id)
        if cached and Path(cached['file_path']).exists():
            # Update last accessed
            self.db.update_access_time(publication.id)
            return LocalFileResult(
                found=True,
                file_path=Path(cached['file_path']),
                file_type=cached['file_type'],
                source=cached['file_source'],
                size_bytes=cached['file_size_bytes']
            )

        # SLOW PATH: Scan filesystem
        result = self._scan_filesystem(publication)

        if result.found:
            # Cache in DB for next time
            self.db.add_cached_file(
                publication_id=publication.id,
                file_path=str(result.file_path),
                file_type=result.file_type,
                file_source=result.source,
                file_hash=self._compute_hash(result.file_path),
                file_size_bytes=result.size_bytes
            )

        return result

    def save_file(self, content, publication, source, file_type='pdf'):
        """Save file and record in database."""

        # Compute hash for deduplication
        file_hash = hashlib.sha256(content).hexdigest()

        # Check if already exists
        existing = self.db.get_by_hash(file_hash)
        if existing:
            logger.info(f"Duplicate detected: {file_hash[:8]}... already at {existing['file_path']}")
            # Add reference to existing file
            self.db.add_duplicate_reference(file_hash, publication.id)
            return Path(existing['file_path'])

        # Save new file
        file_path = self._get_save_path(publication, source, file_type)
        file_path.write_bytes(content)

        # Record in database
        self.db.add_cached_file(
            publication_id=publication.id,
            file_path=str(file_path),
            file_type=file_type,
            file_source=source,
            file_hash=file_hash,
            file_size_bytes=len(content)
        )

        return file_path
```

---

## Comparison Matrix

| Feature | Source-Based | Flat | Hash-Based | Hybrid | DB-Centric |
|---------|-------------|------|------------|--------|------------|
| **Lookup Speed** | Medium (7 dirs) | Fast (1 dir) | Very Fast (O(1)) | Fast (predictable) | Very Fast (indexed) |
| **Provenance** | âœ… Excellent | âŒ Lost | âš ï¸ DB-dependent | âœ… Excellent | âœ… Excellent |
| **Deduplication** | âŒ Manual | âœ… Auto | âœ… Auto | âš ï¸ Manual/DB | âœ… Auto |
| **Scale (1M files)** | âš ï¸ OK | âŒ Slow | âœ… Excellent | âœ… Excellent | âœ… Excellent |
| **Human Readable** | âœ… Yes | âœ… Yes | âŒ No | âœ… Yes | âš ï¸ DB-dependent |
| **Legal Separation** | âœ… Easy | âŒ Hard | âŒ Hard | âœ… Easy | âœ… Easy |
| **Debugging** | âœ… Easy | âŒ Hard | âš ï¸ DB-dependent | âœ… Easy | âœ… Easy |
| **Backup Complexity** | âœ… Simple | âœ… Simple | âš ï¸ Medium | âœ… Simple | âš ï¸ Medium |
| **Migration Risk** | âœ… Low | âœ… Low | âš ï¸ Medium | âœ… Low | âš ï¸ Medium |
| **Implementation** | âœ… Simple | âœ… Simple | âš ï¸ Complex | âš ï¸ Medium | âš ï¸ Complex |

---

## Performance Analysis

### Lookup Performance (1M papers)

**Source-Based (Current Plan):**
```
Average lookup: 7 stat() calls = ~7ms
Worst case: 7 directories to check
Best case: XML found first = 1ms
Cache hit: Database lookup = 0.1ms
```

**Flat Structure:**
```
Average lookup: 1 stat() call = 1ms
BUT: ls on 1M directory = 30+ seconds
Filesystem degradation at 100k+ files
```

**Hash-Based:**
```
Average lookup: 1 stat() call (if hash known) = 1ms
Calculate hash: 100MB PDF = ~200ms (expensive!)
With DB cache: 0.1ms
```

**Hybrid (Source + DB):**
```
Average lookup: 0.1ms (DB cached)
DB miss + filesystem: 2-3ms
No filesystem degradation (subdirs keep it fast)
```

### Storage Efficiency (1M papers)

**Scenario:** 1M papers, 30% duplicates (same paper from multiple sources)

**Source-Based (No Dedup):**
```
Total files: 1M + 300k duplicates = 1.3M files
Average size: 5MB per PDF
Total storage: 1.3M * 5MB = 6.5TB
Wasted: 1.5TB (30% duplication)
```

**With Deduplication (Hash-Based or DB-Centric):**
```
Total unique files: 1M files
Total storage: 1M * 5MB = 5TB
Saved: 1.5TB (23% reduction)
```

---

## Migration Path

### Phase 1: Current (Source-Based) â† **WE ARE HERE**
```
âœ… Simple to implement
âœ… Easy to debug
âœ… Clear provenance
âš ï¸ No deduplication
âš ï¸ 7 directories to check
```

### Phase 2: Add Database Layer (Week 2-3)
```python
# Add metadata.db
# Index all existing files
# Use DB for fast lookups
# Still source-based storage

âœ… Fast lookups (DB cached)
âœ… Deduplication detection
âœ… Analytics ready
âš ï¸ Need to maintain DB
```

### Phase 3: Optimize Storage (Month 2-3)
```python
# Option A: Keep source-based, add symlinks for duplicates
# Option B: Migrate to hash-based storage
# Option C: Keep source-based, delete confirmed duplicates

âœ… Space savings
âœ… Faster queries
âš ï¸ Migration complexity
```

---

## Real-World Examples

### PubMed Central (PMC)
```
PMC uses:
- Source-based (bulk/ vs individual/)
- Subdirectories by ID range (PMC9876/PMC9876543.xml)
- Database metadata (Entrez)

Why: Scale (7M+ articles), provenance tracking
```

### arXiv
```
arXiv uses:
- Year/month subdirectories (2301/2301.12345.pdf)
- Source files separate (2301/2301.12345.tar.gz)
- Database metadata

Why: Time-based access patterns, version tracking
```

### Internet Archive
```
Archive.org uses:
- Content-addressable (hash-based)
- Metadata in database
- Multiple replicas

Why: Maximum deduplication, integrity verification
```

### Our Use Case
```
We need:
- Provenance (source tracking) âœ… Source-based
- Legal separation (Sci-Hub) âœ… Source-based
- Scale (1M+ papers) âœ… Database
- Fast lookups âœ… Database
- Deduplication âœ… Database
- Analytics âœ… Database

Best: Source-based storage + Database metadata
```

---

## Final Recommendation

### ğŸ¯ **Recommended: Source-Based + Database (Hybrid)**

```
data/fulltext/
â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ arxiv/{arxiv_id}.pdf
â”‚   â”œâ”€â”€ pmc/PMC{id}.pdf
â”‚   â”œâ”€â”€ institutional/{doi}.pdf
â”‚   â”œâ”€â”€ publisher/{doi}.pdf
â”‚   â”œâ”€â”€ scihub/{doi}.pdf
â”‚   â””â”€â”€ biorxiv/{doi}.pdf
â”œâ”€â”€ xml/
â”‚   â””â”€â”€ pmc/PMC{id}.nxml
â”œâ”€â”€ parsed/
â”‚   â””â”€â”€ {pub_id}.json
â””â”€â”€ metadata/
    â””â”€â”€ fulltext.db          # Fast lookups + analytics
```

### Why This Structure?

**1. Provenance (Critical for Research)**
- Can cite source: "Full-text obtained via institutional access"
- Legal compliance: Easy to remove Sci-Hub files if needed
- Quality tracking: "Institutional access has 99% success rate"

**2. Debugging & Monitoring**
- "Show me all arXiv downloads" = `ls data/fulltext/pdf/arxiv/ | wc -l`
- "Check institutional access files" = simple directory check
- Source-specific issues easy to identify

**3. Scalability**
- Database indexes handle 1M+ papers easily
- Source-based dirs keep each folder manageable (<100k files each)
- No filesystem degradation

**4. Implementation**
- âœ… Already implemented (SmartCache.py)
- âœ… Database layer = 1 week of work
- âœ… Backwards compatible

**5. Flexibility**
- Can migrate to hash-based later if needed
- Can add deduplication without restructuring
- Can add new sources without breaking existing code

### Implementation Timeline

**Week 1 (Current):** âœ… Source-based storage + SmartCache
**Week 2:** ğŸ“‹ Add SQLite database for metadata
**Week 3:** ğŸš€ Add deduplication detection (via DB)
**Week 4:** ğŸ“Š Add analytics dashboard
**Month 2:** âš¡ Optimize based on real-world usage patterns

---

## Alternative: If You Want Maximum Simplicity

### Ultra-Simple (Flat + Database)

```
data/fulltext/
â”œâ”€â”€ files/
â”‚   â””â”€â”€ {file_hash}.blob    # All files, no structure
â””â”€â”€ fulltext.db             # Maps pub_id â†’ hash + metadata
```

**Pros:**
- Automatic deduplication
- Simplest code
- Fastest lookups

**Cons:**
- Lost human readability
- Database dependency critical
- Harder to debug

**Use if:** You value simplicity over provenance

---

## Conclusion

**Your question is excellent** - the source-based structure IS slightly more complex than alternatives.

**However, for a research tool, provenance matters:**
- Legal: Need to track Sci-Hub vs institutional
- Citations: Need to cite data sources
- Debugging: Need to monitor source effectiveness
- Compliance: Need to delete questionable sources if required

**Recommended: Stick with source-based storage + add database layer next week**

This gives you:
- âœ… Clear provenance (research requirement)
- âœ… Fast lookups (database cached)
- âœ… Deduplication (database detected)
- âœ… Easy debugging (source directories)
- âœ… Legal compliance (delete scihub/ if needed)

**Alternative if you prefer:** Go database-centric from day 1, but you'll need robust backup strategy.

Want me to implement the database layer next, or stick with filesystem-only for now?
