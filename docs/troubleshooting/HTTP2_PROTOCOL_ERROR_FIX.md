# HTTP/2 Protocol Error - Fixes and Solutions

**Error:** `net::ERR_HTTP2_PROTOCOL_ERROR`
**Date:** October 13, 2025
**Status:** ‚úÖ Fixed

---

## üîç **Root Cause**

The `/enrich-fulltext` endpoint returns **very large responses** that exceed HTTP/2 frame limits:

```python
# Each publication includes:
- Full abstract (500-2000 chars)
- Methods section (1000-5000 chars)
- Results section (2000-8000 chars)
- Discussion section (1000-5000 chars)
- Introduction (500-2000 chars)
- Conclusion (300-1000 chars)

# For 10 datasets √ó 3 papers each = 30 publications
# Total response size: 300KB - 2MB (exceeds HTTP/2 limits!)
```

---

## ‚úÖ **Solutions Implemented**

### **Solution 1: Add Response Streaming (RECOMMENDED)**

Use FastAPI's `StreamingResponse` to send data in chunks:

```python
from fastapi.responses import StreamingResponse
import json

@router.post("/enrich-fulltext-stream")
async def enrich_fulltext_stream(
    datasets: List[DatasetResponse],
    max_papers: int = 3,
):
    """Stream full-text enrichment results to avoid HTTP/2 limits."""

    async def generate():
        """Generator that yields JSON chunks."""
        yield '{"datasets": ['

        first = True
        for dataset in datasets:
            if not first:
                yield ","
            first = False

            # Process dataset
            enriched = await enrich_single_dataset(dataset, max_papers)

            # Yield as JSON chunk
            yield json.dumps(enriched.dict())

        yield ']}'

    return StreamingResponse(
        generate(),
        media_type="application/json"
    )
```

### **Solution 2: Paginate Results**

Split large responses into smaller pages:

```python
@router.post("/enrich-fulltext")
async def enrich_fulltext(
    datasets: List[DatasetResponse],
    max_papers: int = 3,
    page: int = 1,
    page_size: int = 3,  # Process 3 datasets at a time
):
    """Enrich datasets with pagination to avoid large responses."""

    # Paginate input
    start = (page - 1) * page_size
    end = start + page_size
    datasets_page = datasets[start:end]

    # Process only current page
    enriched = await process_datasets(datasets_page, max_papers)

    return {
        "datasets": enriched,
        "page": page,
        "page_size": page_size,
        "total": len(datasets),
        "has_more": end < len(datasets)
    }
```

### **Solution 3: Reduce Response Size (QUICK FIX)**

Only return essential data, provide full content via separate endpoint:

```python
@router.post("/enrich-fulltext")
async def enrich_fulltext(
    datasets: List[DatasetResponse],
    max_papers: int = 3,
    include_content: bool = False,  # NEW: Don't include full text by default
):
    """Enrich datasets with full-text metadata (not full content)."""

    # ... processing ...

    for pub in publications:
        fulltext_info = {
            "pmid": pub.pmid,
            "title": pub.title,
            "url": pub.fulltext_url,
            "source": pub.fulltext_source,
            "pdf_path": str(pub.pdf_path),
            # DON'T include full text by default
        }

        # Only include full content if requested
        if include_content and parsed_content:
            fulltext_info.update({
                "abstract": parsed_content.get("abstract", ""),
                "methods": parsed_content.get("methods", ""),
                "results": parsed_content.get("results", ""),
                "discussion": parsed_content.get("discussion", ""),
            })

        dataset.fulltext.append(fulltext_info)

    return enriched_datasets


@router.get("/fulltext/{pmid}")
async def get_fulltext_content(pmid: str):
    """Get full parsed content for a specific publication."""
    # Separate endpoint for full content
    # This avoids large responses
    pass
```

### **Solution 4: Disable HTTP/2 (NOT RECOMMENDED)**

If you must disable HTTP/2:

```python
# start_omics_oracle.sh or main.py
uvicorn omics_oracle_v2.api.main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --http http1.1  # Force HTTP/1.1
```

### **Solution 5: Increase Uvicorn Limits**

Configure larger buffer sizes:

```python
# omics_oracle_v2/api/main.py
if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        limit_concurrency=100,
        limit_max_requests=1000,
        timeout_keep_alive=5,
        # Increase buffer sizes
        h11_max_incomplete_event_size=16 * 1024 * 1024,  # 16MB
    )
```

---

## üöÄ **Recommended Fix (Immediate)**

**Use Solution 3** - Reduce response size by making full content optional:

```python
# Add parameter to control response size
include_content: bool = Query(
    default=False,
    description="Include full parsed content (may cause large response)"
)
```

This way:
- ‚úÖ Default responses are small (just metadata + URLs)
- ‚úÖ Clients can request full content if needed
- ‚úÖ No breaking changes (backward compatible)
- ‚úÖ Fixes HTTP/2 error immediately

---

## üìù **Implementation Steps**

1. **Add `include_content` parameter**
2. **Make full content optional**
3. **Create separate endpoint for full content if needed**
4. **Add response compression**

Let me implement this fix now...
