# OmicsOracle: Actual Implementation & End-to-End Flow

**Date:** October 10, 2025
**Status:** Current State Documentation

---

## ğŸ¯ Overview

This document describes the **actual implemented flow** based on the codebase, not theoretical pipelines. It clarifies what works today and what's planned for the future.

---

## ğŸ“Š Current Architecture: Two Main Workflows

### Workflow 1: GEO Search via Web UI/API
**Entry Point:** User enters query in frontend
**Current Status:** âœ… **FULLY IMPLEMENTED**

### Workflow 2: GEO â†’ Citations â†’ PDFs Collection
**Entry Point:** Programmatic via `GEOCitationPipeline`
**Current Status:** âœ… **FULLY IMPLEMENTED** (Collection phase only)

---

## ğŸ” WORKFLOW 1: Publication Search via Streamlit Dashboard (ACTUAL!)

### âš ï¸ CRITICAL DISCOVERY: Dashboard Uses PublicationSearchPipeline, NOT SearchAgent!

**The actual flow is completely different from what I initially described!**

### End-to-End Flow

```
User Query (Streamlit Dashboard)
    â†“
Dashboard SearchPanel.render()
    â†“
User clicks "Search" button
    â†“
DashboardApp._execute_search()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Query Preprocessing         â”‚
â”‚  (if enable_query_preprocessing)     â”‚
â”‚  - BiomedicalNER entity extraction   â”‚
â”‚  - Extract: Genes, Diseases,         â”‚
â”‚    Techniques, Organisms             â”‚
â”‚  - Build source-specific queries:    â”‚
â”‚    * PubMed: Add field tags          â”‚
â”‚      [Gene Name], [MeSH], etc.       â”‚
â”‚    * OpenAlex: Prioritize entities   â”‚
â”‚    * Scholar: Use expanded query     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Synonym Expansion           â”‚
â”‚  (if enable_synonym_expansion)       â”‚
â”‚  - SynonymExpander.expand_query()    â”‚
â”‚  - Use ontology gazetteers           â”‚
â”‚  - Add technique synonyms            â”‚
â”‚    Example: "HiC" â†’ "Hi-C", "3C"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Multi-Source Search         â”‚
â”‚  - PubMed (if enable_pubmed)         â”‚
â”‚    Uses optimized query with tags    â”‚
â”‚  - OpenAlex (if enable_openalex)     â”‚
â”‚    Prioritizes entity terms          â”‚
â”‚  - Google Scholar (if enable_scholar)â”‚
â”‚    Uses expanded query               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Deduplication               â”‚
â”‚  - Pass 1: ID-based (PMID, DOI)      â”‚
â”‚  - Pass 2: Fuzzy matching            â”‚
â”‚    Title/author similarity           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Institutional Access        â”‚
â”‚  (if enable_institutional_access)    â”‚
â”‚  - Check Georgia Tech/ODU access     â”‚
â”‚  - Add access_url to metadata        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: Full-Text URL Collection    â”‚
â”‚  (if enable_fulltext_retrieval)      â”‚
â”‚  - FullTextManager waterfall:        â”‚
â”‚    1. CORE                           â”‚
â”‚    2. BioRxiv                        â”‚
â”‚    3. ArXiv                          â”‚
â”‚    4. CrossRef                       â”‚
â”‚    5. OpenAlex OA URLs               â”‚
â”‚    6. Unpaywall                      â”‚
â”‚    7. Sci-Hub (optional)             â”‚
â”‚    8. LibGen (optional)              â”‚
â”‚  - Add fulltext_url to metadata      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 7: Ranking                     â”‚
â”‚  - PublicationRanker.rank()          â”‚
â”‚  - Multi-factor scoring              â”‚
â”‚  - Return top N results              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 8: Citation Enrichment         â”‚
â”‚  (if enable_citations)               â”‚
â”‚  - Find citing papers (CitationFinder)â”‚
â”‚    Uses OpenAlex + Scholar + S2      â”‚
â”‚  - LLM analysis of citations         â”‚
â”‚  - Dataset reuse detection           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 9: Semantic Scholar Enrichment â”‚
â”‚  - Add citation counts               â”‚
â”‚  - Add influence scores              â”‚
â”‚  - Free alternative to Scholar       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 10: PDF Download (Optional)    â”‚
â”‚  (if enable_pdf_download)            â”‚
â”‚  - PDFDownloadManager                â”‚
â”‚  - Async parallel downloads          â”‚
â”‚  - Validation & retries              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 11: Text Extraction (Optional) â”‚
â”‚  (if enable_fulltext)                â”‚
â”‚  - PDFTextExtractor                  â”‚
â”‚  - Extract text from downloaded PDFs â”‚
â”‚  - Add full_text to Publication      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Results Displayed in Dashboard
    â†“
Tabs: Results | Visualizations | Analytics
```

### Actual Code Components (Dashboard â†’ PublicationSearchPipeline)

#### 1. **Frontend: Streamlit Dashboard**
- Location: `omics_oracle_v2/lib/dashboard/app.py`
- Framework: Streamlit (NOT static HTML!)
- Port: 8502 (default)

**Code:**
```python
# dashboard/app.py line ~150
def _execute_search(self, params: Dict[str, Any]) -> None:
    """Execute search with given parameters."""
    query = params["query"]

    with st.spinner(f"Searching for: {query}..."):
        # Import PublicationSearchPipeline (NOT SearchAgent!)
        from omics_oracle_v2.lib.pipelines.publication_pipeline import PublicationSearchPipeline
        from omics_oracle_v2.lib.publications.config import PublicationSearchConfig

        # Create pipeline config
        pipeline_config = PublicationSearchConfig(
            enable_pubmed="pubmed" in params["databases"],
            enable_scholar="scholar" in params["databases"],
            enable_citations=params.get("use_llm", False),
            max_total_results=params["max_results"],
        )

        # Execute search via pipeline
        pipeline = PublicationSearchPipeline(pipeline_config)
        search_result = pipeline.search(
            query=query,
            max_results=params["max_results"],
        )
```

**Key Discovery:** Dashboard uses `PublicationSearchPipeline`, not `SearchAgent`!

#### 2. **PublicationSearchPipeline** (Main Orchestrator)
- File: `omics_oracle_v2/lib/pipelines/publication_pipeline.py`
- 11-step pipeline with conditional execution based on feature toggles

**Main Search Method:**
```python
# publication_pipeline.py line ~470
def search(self, query: str, max_results: int = 50, **kwargs) -> PublicationResult:
    """Search for publications across enabled sources."""

    # Step 0: Preprocess query (NEW - Phase 1)
    preprocessed = self._preprocess_query(query)
    # Extracts entities, expands synonyms, builds source-specific queries

    # Step 1: Search enabled sources
    all_publications = []

    # 1a. PubMed (conditional)
    if self.pubmed_client:
        pubmed_query = preprocessed.get("pubmed", query)
        pubmed_results = self.pubmed_client.search(pubmed_query, max_results)
        all_publications.extend(pubmed_results)

    # 1b. Google Scholar (conditional)
    if self.scholar_client:
        scholar_query = preprocessed.get("scholar", query)
        scholar_results = self.scholar_client.search(scholar_query, max_results)
        all_publications.extend(scholar_results)

    # 1c. OpenAlex (conditional)
    if self.openalex_client:
        openalex_query = preprocessed.get("openalex", query)
        openalex_results = self.openalex_client.search(openalex_query, max_results)
        all_publications.extend(openalex_results)

    # Step 2: Deduplicate (2-pass: ID-based + fuzzy)
    all_publications = self._deduplicate_publications(all_publications)

    # Step 3: Institutional access enrichment
    if self.institutional_manager:
        # Add access_url, access_status to metadata
        # ...

    # Step 3.5: Full-text URL enrichment
    if self.fulltext_manager:
        # Waterfall through 8 sources to find PDFs
        # ...

    # Step 4: Rank publications
    ranked_results = self.ranker.rank(all_publications, query, max_results)

    # Step 5: Citation enrichment
    if self.citation_finder:
        # Find citing papers using OpenAlex/Scholar/S2
        ranked_results = self._enrich_citations(ranked_results)

    # Step 5.5: Semantic Scholar enrichment
    if self.semantic_scholar_client:
        # Add citation counts, influence scores
        # ...

    # Step 6: PDF download
    if self.pdf_downloader:
        self._download_pdfs(ranked_results)

    # Step 7: Text extraction
    if self.pdf_text_extractor:
        ranked_results = self._extract_fulltext(ranked_results)

    return PublicationResult(
        query=query,
        publications=ranked_results,
        sources_used=sources_used,
        # ...
    )
```

#### 3. **Query Preprocessing** (NEW! - This is what I missed!)
- File: `omics_oracle_v2/lib/pipelines/publication_pipeline.py`
- Uses: BiomedicalNER + SynonymExpander

**Code:**
```python
# publication_pipeline.py line ~290
def _preprocess_query(self, query: str) -> dict:
    """
    Preprocess query to extract biological entities and build optimized queries.

    Phase 1: Basic entity extraction + field tagging
    Phase 2B: Synonym expansion with ontologies
    """
    # Step 1: Synonym expansion
    expanded_query = query
    if self.synonym_expander:
        expanded_query = self.synonym_expander.expand_query(query)
        # Example: "DNA methylation HiC" â†’ "DNA methylation (HiC OR Hi-C OR 3C)"

    # Step 2: Entity extraction
    if self.ner:
        ner_result = self.ner.extract_entities(expanded_query)
        entities_by_type = ner_result.entities_by_type
        # Extracts: GENE, DISEASE, TECHNIQUE, ORGANISM

    # Step 3: Build source-specific queries
    return {
        "original": query,
        "expanded": expanded_query,
        "entities": entities_by_type,
        "pubmed": self._build_pubmed_query(expanded_query, entities_by_type),
        "openalex": self._build_openalex_query(expanded_query, entities_by_type),
        "scholar": expanded_query,
    }
```

**PubMed Query Builder:**
```python
# publication_pipeline.py line ~320
def _build_pubmed_query(self, original_query: str, entities_by_type: dict) -> str:
    """Build PubMed-optimized query with field tags."""
    parts = []

    # Add genes with field tag
    if EntityType.GENE in entities_by_type:
        genes = entities_by_type[EntityType.GENE]
        gene_terms = " OR ".join(f'"{g.text}"[Gene Name]' for g in genes[:5])
        parts.append(f"({gene_terms})")

    # Add diseases with MeSH tag
    if EntityType.DISEASE in entities_by_type:
        diseases = entities_by_type[EntityType.DISEASE]
        disease_terms = " OR ".join(f'"{d.text}"[MeSH]' for d in diseases[:5])
        parts.append(f"({disease_terms})")

    # Combine enhanced query with original
    if parts:
        enhanced = " AND ".join(parts)
        return f"({enhanced}) OR ({original_query})"

    return original_query
```

**Real Example:**
```
Input:  "breast cancer BRCA1"
â†“ Synonym Expansion
"breast cancer (BRCA1 OR BRCA1 gene)"
â†“ Entity Extraction
Entities: DISEASE=["breast cancer"], GENE=["BRCA1"]
â†“ PubMed Query
("breast cancer"[MeSH]) AND ("BRCA1"[Gene Name] OR "BRCA1 gene"[Gene Name]) OR (breast cancer BRCA1)
```

#### 4. **Multi-Source Search Clients**

**PubMed Client:**
- File: `omics_oracle_v2/lib/publications/clients/pubmed.py`
- Uses NCBI E-utilities API with optimized queries

**OpenAlex Client:**
- File: `omics_oracle_v2/lib/citations/clients/openalex.py`
- Free, sustainable alternative to Google Scholar
- 250M+ works, citation data, open access URLs

**Google Scholar Client:**
- File: `omics_oracle_v2/lib/citations/clients/scholar.py`
- Fallback for citations and hard-to-find papers

#### 5. **Deduplication System** (2-pass)
- File: `omics_oracle_v2/lib/publications/deduplication.py`

**Code:**
```python
def _deduplicate_publications(self, publications: List[Publication]) -> List[Publication]:
    """
    Multi-pass deduplication:
    - Pass 1: ID-based (PMID, DOI) - exact matching
    - Pass 2: Fuzzy (title, authors, year) - catches variations
    """
    # Pass 1: ID-based
    seen_pmids, seen_dois = set(), set()
    unique_pubs = []
    for pub in publications:
        if pub.pmid not in seen_pmids and pub.doi not in seen_dois:
            unique_pubs.append(pub)
            seen_pmids.add(pub.pmid)
            seen_dois.add(pub.doi)

    # Pass 2: Fuzzy matching (if enabled)
    if self.fuzzy_deduplicator:
        unique_pubs = self.fuzzy_deduplicator.deduplicate(unique_pubs)

    return unique_pubs
```

#### 6. **Full-Text URL Collection** (8-source waterfall)
- File: `omics_oracle_v2/lib/fulltext/manager.py`

**Sources (in priority order):**
1. CORE - Academic papers
2. BioRxiv - Preprints
3. ArXiv - Scientific preprints
4. CrossRef - DOI metadata
5. OpenAlex - OA URLs from metadata
6. Unpaywall - Legal OA repository (50% improvement)
7. Sci-Hub - Mirror repository (25% improvement, optional)
8. LibGen - Document repository (5-10% improvement, optional)

### Key Differences from Initial Understanding

âœ… **HAS query preprocessing** (BiomedicalNER + SynonymExpander)
- Entity extraction (genes, diseases, techniques)
- Synonym expansion with ontologies
- Source-specific query optimization

âœ… **HAS multi-source search** (PubMed + OpenAlex + Scholar)
- Not just GEO datasets
- Publication search across databases
- 250M+ papers available

âœ… **HAS advanced deduplication** (2-pass system)
- ID-based exact matching
- Fuzzy title/author matching
- Handles preprint/published pairs

âœ… **HAS full-text collection** (8-source waterfall)
- Institutional access first
- Multiple OA sources
- Optional mirror sites

âŒ **Dashboard does NOT use SearchAgent**
- Uses PublicationSearchPipeline directly
- SearchAgent is for GEO datasets only (different use case)

---

## ğŸ§¬ WORKFLOW 2: GEO Search via API (SearchAgent)

**This is a SEPARATE workflow from the dashboard!**

### End-to-End Flow

```
API Request
    â†“
POST /api/agents/search
    â†“
SearchAgent.execute()
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GEO Client Search                   â”‚
â”‚  - Query sent AS-IS to NCBI          â”‚
â”‚  - NO preprocessing                  â”‚
â”‚  - NO synonym expansion              â”‚
â”‚  - Esearch + Esummary                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Semantic Search (Optional)          â”‚
â”‚  - If enable_semantic=true           â”‚
â”‚  - FAISS vector search               â”‚
â”‚  - Hybrid TF-IDF + vector ranking    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7-Dimension Quality Scoring         â”‚
â”‚  1. Sample count                     â”‚
â”‚  2. Metadata completeness            â”‚
â”‚  3. Recency                          â”‚
â”‚  4. Citation count                   â”‚
â”‚  5. Platform diversity               â”‚
â”‚  6. Data availability                â”‚
â”‚  7. Publication status               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Return JSON Response
```

### Code Implementation

**API Endpoint:**
```python
# omics_oracle_v2/api/routes/agents.py
@router.post("/search")
async def execute_search_agent(request: SearchRequest):
    agent = SearchAgent(settings, enable_semantic=request.enable_semantic)

    search_input = SearchInput(
        search_terms=request.search_terms,
        filters=request.filters,
        max_results=request.max_results
    )

    result = agent.execute(search_input)
    return SearchResponse(datasets=ranked_datasets)
```

**GEO Client:**
```python
# omics_oracle_v2/lib/geo/client.py
async def search(self, query: str, max_results: int) -> GEOSearchResult:
    # Direct NCBI E-utilities call - NO preprocessing!
    esearch_url = f"{BASE_URL}/esearch.fcgi"
    params = {
        "db": "gds",
        "term": query,  # â† Query used AS-IS
        "retmax": max_results,
        "retmode": "json"
    }
    # ...
```

### Key Differences from Dashboard Workflow

| Feature | Dashboard (PublicationSearchPipeline) | API (SearchAgent) |
|---------|--------------------------------------|-------------------|
| **Query Preprocessing** | âœ… YES (BiomedicalNER + Synonyms) | âŒ NO (direct to NCBI) |
| **Target Database** | PubMed, OpenAlex, Scholar | GEO datasets only |
| **Data Type** | Publications (papers) | GEO Series (datasets) |
| **Citation Discovery** | âœ… YES (multi-source) | âŒ NO |
| **PDF Download** | âœ… YES (optional) | âŒ NO |
| **Full-text Collection** | âœ… YES (8 sources) | âŒ NO |
| **Use Case** | Find papers on a topic | Find GEO datasets |

---

## ğŸ“š WORKFLOW 3: GEO Citation Pipeline (Programmatic)

### End-to-End Flow

```
Python Script Call
    â†“
pipeline = GEOCitationPipeline(config)
result = await pipeline.collect(query="breast cancer RNA-seq")
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Query Optimization          â”‚
â”‚  - GEOQueryBuilder                   â”‚
â”‚  - Extract key scientific terms      â”‚
â”‚  - Remove stop words                 â”‚
â”‚  - Add field restrictions [Title]    â”‚
â”‚  - Create Boolean query with OR      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: GEO Search                  â”‚
â”‚  - Use optimized query               â”‚
â”‚  - GEOClient.search()                â”‚
â”‚  - Batch metadata fetching (parallel)â”‚
â”‚  - Returns: List<GEOSeriesMetadata>  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Citation Discovery          â”‚
â”‚  - For each GEO dataset:             â”‚
â”‚    - Strategy 1: Find papers         â”‚
â”‚      citing original publication     â”‚
â”‚    - Strategy 2: Find papers         â”‚
â”‚      mentioning GEO ID (e.g. GSE123)â”‚
â”‚  - Deduplicate by PMID/DOI          â”‚
â”‚  - Returns: List<Publication>        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Full-Text URL Collection    â”‚
â”‚  - FullTextManager (waterfall)       â”‚
â”‚  - Try sources in order:             â”‚
â”‚    1. Institutional access           â”‚
â”‚    2. Unpaywall                      â”‚
â”‚    3. CORE                           â”‚
â”‚    4. Sci-Hub (optional)             â”‚
â”‚    5. LibGen (optional)              â”‚
â”‚  - Add fulltext_url to Publication   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: PDF Download                â”‚
â”‚  - PDFDownloadManager                â”‚
â”‚  - Parallel downloads (5 concurrent) â”‚
â”‚  - Validation & retry logic          â”‚
â”‚  - Save to: data/geo_citation_       â”‚
â”‚    collections/{query}_{timestamp}/  â”‚
â”‚    pdfs/                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: Metadata Storage            â”‚
â”‚  - Save to collection directory:     â”‚
â”‚    - geo_datasets.json               â”‚
â”‚    - citing_papers.json              â”‚
â”‚    - collection_report.json          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Return CollectionResult
```

### Actual Code Implementation

#### Step 1: Query Optimization

**File:** `omics_oracle_v2/lib/geo/query_builder.py`

```python
class GEOQueryBuilder:
    def build_query(self, query: str, mode="balanced") -> str:
        """
        Transform natural language to optimized NCBI query.

        Example:
        Input:  "DNA methylation HiC"
        Output: ("DNA"[Title] OR "methylation"[Title]) AND
                (HiC[Title] OR "Hi-C"[Title] OR "3C"[Title])
        """
        # 1. Extract key terms (remove stop words)
        terms = self._extract_key_terms(query)

        # 2. Expand technique synonyms
        expanded_terms = self._expand_synonyms(terms)

        # 3. Build Boolean query with field restrictions
        if mode == "comprehensive":
            # Search in Title OR Description
            query_parts = [
                f'({term}[Title] OR {term}[Description])'
                for term in expanded_terms
            ]
        else:  # balanced (default)
            # Title-only for precision
            query_parts = [f'{term}[Title]' for term in expanded_terms]

        return " AND ".join(query_parts)
```

**Real Example:**
```
Input:  "DNA methylation HiC"
Output: ("DNA"[Title] OR "methylation"[Title]) AND
        (HiC[Title] OR "Hi-C"[Title])

Result: Found 18 datasets (vs 1 with naive query)
```

#### Step 2: GEO Search (Same as Workflow 1)

Uses `GEOClient` but with optimized query from Step 1.

#### Step 3: Citation Discovery

**File:** `omics_oracle_v2/lib/citations/discovery/geo_discovery.py`

```python
class GEOCitationDiscovery:
    async def find_citing_papers(
        self,
        geo_metadata: GEOSeriesMetadata,
        max_results: int = 100
    ) -> CitationDiscoveryResult:
        """
        Find papers citing this GEO dataset.

        Two strategies:
        1. Citation-based: Papers citing original publication
        2. Mention-based: Papers mentioning GEO ID in text
        """
        citing_papers = []

        # Strategy 1: Citation discovery
        if geo_metadata.pmid:
            cited_by = await self._find_citing_via_pubmed(
                geo_metadata.pmid
            )
            citing_papers.extend(cited_by)

        # Strategy 2: Mention discovery
        mentioned_in = await self._find_mentioning_geo_id(
            geo_metadata.geo_id
        )
        citing_papers.extend(mentioned_in)

        # Deduplicate
        unique_papers = self._deduplicate(citing_papers)

        return CitationDiscoveryResult(
            geo_id=geo_metadata.geo_id,
            citing_papers=unique_papers
        )
```

#### Step 4: Full-Text URL Collection

**File:** `omics_oracle_v2/lib/fulltext/manager.py`

```python
class FullTextManager:
    async def get_fulltext_batch(
        self,
        publications: List[Publication]
    ) -> List[Publication]:
        """
        Add full-text URLs using waterfall strategy.

        Tries sources in priority order until one succeeds.
        """
        for pub in publications:
            # Try each source in order
            for source in self.sources:
                url = await source.get_fulltext_url(pub)
                if url:
                    pub.fulltext_url = url
                    pub.fulltext_source = source.name
                    break  # Stop at first success

        return publications
```

**Sources (in priority order):**
1. **Institutional Access** - Georgia Tech proxy, ezproxy
2. **Unpaywall** - Legal open access repository
3. **CORE** - Academic paper aggregator
4. **Sci-Hub** (optional) - Mirror repository
5. **LibGen** (optional) - Document repository

#### Step 5: PDF Download

**File:** `omics_oracle_v2/lib/storage/pdf/download_manager.py`

```python
class PDFDownloadManager:
    async def download_batch(
        self,
        publications: List[Publication],
        output_dir: Path
    ) -> DownloadReport:
        """
        Download PDFs with parallel processing and retries.
        """
        # Filter publications with URLs
        to_download = [p for p in publications if p.fulltext_url]

        # Download concurrently (max 5 at a time)
        tasks = []
        for pub in to_download:
            task = self._download_single(pub, output_dir)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Validate PDFs
        if self.validate_pdf:
            for result in results:
                if result.success:
                    is_valid = self._validate_pdf(result.file_path)
                    result.validated = is_valid

        return DownloadReport(
            total=len(to_download),
            successful=sum(1 for r in results if r.success),
            failed=sum(1 for r in results if not r.success)
        )
```

#### Step 6: Metadata Storage

**File:** `omics_oracle_v2/lib/pipelines/geo_citation_pipeline.py`

```python
async def _save_metadata(
    self,
    collection_dir: Path,
    query: str,
    datasets: List[GEOSeriesMetadata],
    papers: List[Publication],
    citation_results: List[CitationDiscoveryResult],
    download_report: dict
):
    """
    Save all collected data for future analysis.

    Directory structure:
    data/geo_citation_collections/
        {query}_{timestamp}/
            geo_datasets.json       â† GEO metadata
            citing_papers.json      â† Publication metadata
            collection_report.json  â† Summary stats
            pdfs/                   â† Downloaded PDFs
                {pmid}.pdf
    """
    # Save GEO datasets
    with open(collection_dir / "geo_datasets.json", "w") as f:
        json.dump([asdict(ds) for ds in datasets], f, indent=2)

    # Save citing papers
    with open(collection_dir / "citing_papers.json", "w") as f:
        json.dump([asdict(p) for p in papers], f, indent=2)

    # Save summary report
    report = {
        "query": query,
        "timestamp": datetime.now().isoformat(),
        "datasets_count": len(datasets),
        "citing_papers_count": len(papers),
        "fulltext_coverage": sum(1 for p in papers if p.fulltext_url) / len(papers),
        "pdfs_downloaded": download_report.get("successful", 0),
        "download_report": download_report
    }
    with open(collection_dir / "collection_report.json", "w") as f:
        json.dump(report, f, indent=2)
```

---

## âš ï¸ What's NOT Yet Implemented

### Phase 7: Analysis & Insights (Future Work)

**NOT YET BUILT:**
- âŒ PDF text extraction at scale
- âŒ LLM analysis of collected papers
- âŒ Chat interface over collected documents
- âŒ Report/summary generation
- âŒ Research idea generation
- âŒ Insight extraction

**Current State:**
```python
# geo_citation_pipeline.py line 7:
# NO LLM ANALYSIS - Pure data collection phase.
# Phase 7 will add LLM analysis of collected papers.
```

**What needs to be built:**

```python
# Future: Phase 7 - Analysis Pipeline
class AnalysisPipeline:
    """
    Analyze collected documents and generate insights.

    NOT YET IMPLEMENTED
    """

    async def analyze_collection(
        self,
        collection_dir: Path
    ) -> AnalysisResult:
        """
        1. Extract text from PDFs
        2. Chunk documents for LLM context
        3. Generate embeddings
        4. Enable RAG Q&A
        5. Generate summaries
        6. Extract key findings
        7. Suggest research directions
        """
        pass  # TODO: Implement Phase 7
```

---

## ğŸ¯ Summary: What Actually Works Today

### âœ… Implemented & Working

**Dashboard Publication Search (Workflow 1):**
- âœ… Query preprocessing (BiomedicalNER + SynonymExpander)
- âœ… Entity extraction (genes, diseases, techniques, organisms)
- âœ… Synonym expansion with ontologies
- âœ… Source-specific query optimization (PubMed field tags, OpenAlex prioritization)
- âœ… Multi-source search (PubMed + OpenAlex + Google Scholar)
- âœ… 2-pass deduplication (ID-based + fuzzy matching)
- âœ… Institutional access checking (Georgia Tech + ODU)
- âœ… Full-text URL collection (8-source waterfall)
- âœ… Advanced ranking system
- âœ… Citation enrichment (OpenAlex + Scholar + Semantic Scholar)
- âœ… PDF download (async, parallel, validated)
- âœ… Text extraction from PDFs
- âœ… Streamlit dashboard with visualizations

**GEO Search API (Workflow 2):**
- âœ… Direct NCBI GEO search
- âœ… 7-dimension quality scoring
- âœ… Optional semantic search (FAISS)
- âœ… JSON API endpoint

**GEO Citation Collection (Workflow 3):**
- âœ… Query optimization (GEOQueryBuilder)
- âœ… GEO dataset discovery
- âœ… Citation discovery (2 strategies)
- âœ… Full-text URL collection (5+ sources)
- âœ… PDF download (parallel, validated)
- âœ… Metadata storage (structured JSON)

### âŒ Not Yet Implemented

**Phase 7 (Future - Analysis Pipeline):**
- âŒ LLM-based document analysis at scale
- âŒ RAG Q&A over collected papers
- âŒ Automated summary/report generation
- âŒ Research idea generation from literature
- âŒ Insight extraction and synthesis
- âŒ Trend analysis across documents
- âŒ Chat interface over knowledge base

### ğŸ” Major Discovery Summary

**What I Got Wrong Initially:**
1. âŒ Thought dashboard used SearchAgent â†’ Actually uses PublicationSearchPipeline
2. âŒ Thought there was NO query preprocessing â†’ Actually has sophisticated NER + synonyms
3. âŒ Thought only GEO search was available â†’ Actually has publication search too
4. âŒ Thought workflows were related â†’ They're completely separate use cases

**What I Got Right:**
1. âœ… GEOCitationPipeline is for collection only (no analysis yet)
2. âœ… Future work is analysis/insights generation
3. âœ… Multiple sources with waterfall strategy
4. âœ… Async PDF download and validation

---

## ğŸš€ Recommended Next Steps

Based on this analysis, here's what we should focus on:

### Option A: Complete Workflow 1 (UI/API Enhancement)
**Goal:** Bring citation collection to the web UI

**Tasks:**
1. Add "Collect Citations" button to search results
2. Create API endpoint: `POST /api/citations/collect`
3. Background job for citation pipeline
4. Progress tracking (WebSocket or polling)
5. Display collection results in UI

**Timeline:** 1-2 days

### Option B: Start Phase 7 (Analysis Pipeline)
**Goal:** Extract value from collected documents

**Tasks:**
1. PDF text extraction service
2. Document chunking for LLM
3. Embedding generation
4. RAG interface (Q&A over papers)
5. Summary generation

**Timeline:** 1-2 weeks

### Option C: Fix Flow Diagram (Documentation)
**Goal:** Update docs to reflect actual implementation

**Tasks:**
1. Rewrite PIPELINE_FLOW_DIAGRAM.md
2. Create COLLECTION_VS_ANALYSIS.md
3. Update PIPELINE_DECISION_GUIDE.md
4. Add architecture diagrams

**Timeline:** 2-3 hours

---

## ğŸ’¡ My Recommendation

**Start with Option C** (documentation fix), then **Option A** (UI integration), then **Option B** (analysis).

**Reasoning:**
1. Documentation ensures we have correct mental model
2. UI integration makes the tool usable for end users
3. Analysis phase is the most complex (needs careful design)

**What should we do now?**
- Fix the flow diagram to show actual implementation
- Create a clear separation: Collection vs Analysis
- Plan the Analysis Pipeline architecture

Would you like me to:
1. **Fix the flow diagram** to match actual code?
2. **Plan the Analysis Pipeline** architecture?
3. **Design the UI integration** for citation collection?

Let me know which direction you want to go! ğŸš€
