# Section 1: Current Implementation Deep Dive

## üìä Part 1: Complete Citation Analysis Workflow

### The Full Pipeline (Already Implemented!)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               STAGE 1: GEO DATASET PROCESSING                   ‚îÇ
‚îÇ  (Your starting point - SearchAgent + DataAgent)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        GEO Dataset: GSE123456 with pubmed_ids: ["12345678"]
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          STAGE 2: PUBLICATION METADATA ENRICHMENT               ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/pipeline.py            ‚îÇ
‚îÇ  Location: Lines 180-390                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        Step 2.1: Fetch publication from PubMed (PMID: 12345678)
        Step 2.2: Enrich with Semantic Scholar citations (count)
                            ‚Üì
        Publication object with citation count
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      STAGE 3: CITATION ANALYSIS - GET CITING PAPERS ‚úÖ          ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/pipeline.py            ‚îÇ
‚îÇ  Method: _enrich_citations() - Lines 610-700                    ‚îÇ
‚îÇ  Agent: CitationAnalyzer                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        Uses Google Scholar to find papers that cite dataset paper
                            ‚Üì
        Result: List of 50-100 citing publications
        - Each with title, abstract, DOI, authors, year
        - Citation context (text around citation)
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       STAGE 4: PDF DOWNLOAD & FULL-TEXT EXTRACTION ‚úÖ           ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/pdf_downloader.py      ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/fulltext_extractor.py  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        Step 4.1: Download PDFs from institutional access
        Step 4.2: Extract full text from PDFs
        Step 4.3: Store in local repository
                            ‚Üì
        Result: Full-text documents for citing papers
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    STAGE 5: LLM ANALYSIS - UNDERSTAND DATASET USAGE ‚úÖ          ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/citations/             ‚îÇ
‚îÇ        llm_analyzer.py                                          ‚îÇ
‚îÇ  Agent: LLMCitationAnalyzer                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        GPT-4 analyzes each citing paper to extract:
        - Was dataset reused? (True/False)
        - Usage type (validation, novel_application, etc.)
        - Application domain (cancer, diabetes, etc.)
        - Key findings from the study
        - Novel biomarkers discovered
        - Clinical relevance (high/medium/low)
        - Validation status
                            ‚Üì
        Result: UsageAnalysis objects for each citing paper
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      STAGE 6: INTERACTIVE Q&A SYSTEM ‚úÖ                         ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/analysis/qa_system.py  ‚îÇ
‚îÇ  Agent: DatasetQASystem                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        User asks: "What novel biomarkers were discovered?"
                            ‚Üì
        System:
        1. Searches through citation analyses
        2. Finds relevant papers mentioning biomarkers
        3. Uses GPT-4 to synthesize answer
        4. Returns answer with evidence citations
                            ‚Üì
        Result: Evidence-based answer with source papers
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         STAGE 7: COMPREHENSIVE IMPACT REPORT ‚úÖ                 ‚îÇ
‚îÇ  File: omics_oracle_v2/lib/publications/citations/             ‚îÇ
‚îÇ        llm_analyzer.py                                          ‚îÇ
‚îÇ  Method: synthesize_dataset_impact()                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
        Generates comprehensive report:
        - Total citations & reuse rate
        - Usage types breakdown
        - Application domains
        - All biomarkers discovered
        - Clinical translation status
        - Time span of impact
        - Synthesized narrative summary
```

---

## üìÅ Part 2: File-by-File Implementation Details

### File 1: Citation Analyzer
**Location:** `omics_oracle_v2/lib/publications/citations/analyzer.py`

**Purpose:** Get papers that cite a dataset publication

**Key Methods:**
```python
class CitationAnalyzer:
    def __init__(self, scholar_client: GoogleScholarClient):
        """Uses Google Scholar to find citations"""
        
    def get_citing_papers(
        self, 
        publication: Publication, 
        max_results: int = 100
    ) -> List[Publication]:
        """
        Gets papers that cite the dataset publication.
        
        Returns:
            List of citing publications with:
            - Title, abstract, DOI
            - Authors, year
            - Citation context (text around citation)
        """
        
    def get_citation_contexts(
        self,
        cited_publication: Publication,
        citing_publication: Publication
    ) -> List[CitationContext]:
        """
        Extracts the text around where dataset is cited.
        
        This is the "how they used it" context you want!
        """
        
    def analyze_citation_network(
        self,
        publication: Publication,
        depth: int = 1
    ) -> dict:
        """
        Builds citation network graph.
        
        Returns:
            - Papers citing this dataset
            - Papers those papers cite (2nd level)
            - Network statistics
        """
```

**Status:** ‚úÖ **FULLY IMPLEMENTED**

---

### File 2: PDF Downloader
**Location:** `omics_oracle_v2/lib/publications/pdf_downloader.py`

**Purpose:** Download full-text PDFs of citing papers

**Key Features:**
```python
class PDFDownloader:
    def __init__(self, download_dir: Path, institutional_manager=None):
        """
        Downloads PDFs with institutional access support.
        
        Sources:
        - PubMed Central (free)
        - Unpaywall (open access)
        - Institutional subscriptions (Georgia Tech proxy)
        - Publisher direct links
        """
        
    def download(
        self,
        pdf_url: str,
        identifier: str,
        source: str = "unknown"
    ) -> Optional[Path]:
        """
        Downloads single PDF.
        
        Features:
        - Deduplication (checks if already downloaded)
        - Retry logic (3 attempts)
        - Validation (checks PDF magic bytes)
        - Organized storage (by source)
        
        Returns:
            Path to downloaded PDF or None
        """
        
    def download_batch(
        self,
        publications: List[Publication],
        max_workers: int = 5
    ) -> Dict[str, Path]:
        """
        Concurrent batch download.
        
        Downloads multiple PDFs in parallel.
        """
```

**Download Statistics (from logs):**
- PMC: ~80% success rate (free access)
- Unpaywall: ~40% success rate (open access)
- Institutional: ~60% success rate (subscription)
- Overall: ~70% of papers get full text

**Status:** ‚úÖ **FULLY IMPLEMENTED**

---

### File 3: Full-Text Extractor
**Location:** `omics_oracle_v2/lib/publications/fulltext_extractor.py`

**Purpose:** Extract text from downloaded PDFs

**Implementation:**
```python
class FullTextExtractor:
    """Extracts text from PDF files."""
    
    def extract_text(self, pdf_path: Path) -> Optional[str]:
        """
        Extracts full text from PDF.
        
        Methods:
        1. PyPDF2 (fast, basic extraction)
        2. pdfplumber (better formatting)
        3. OCR fallback (for scanned PDFs)
        
        Returns:
            Full text string or None if failed
        """
        
    def extract_sections(self, pdf_path: Path) -> Dict[str, str]:
        """
        Extracts specific sections (optional enhancement).
        
        Sections:
        - Abstract
        - Methods
        - Results
        - Discussion
        
        Useful for focused analysis!
        """
```

**Status:** ‚úÖ **IMPLEMENTED** (basic extraction working)

---

### File 4: LLM Citation Analyzer
**Location:** `omics_oracle_v2/lib/publications/citations/llm_analyzer.py`

**Purpose:** Use GPT-4 to understand HOW datasets are used in citing papers

**This is the CORE of what you're asking for!**

**Key Class:**
```python
class LLMCitationAnalyzer:
    """
    LLM-powered deep analysis of citations and dataset usage.
    
    Uses GPT-4 to:
    - Understand citation context semantically
    - Classify dataset usage types
    - Extract key findings
    - Identify novel biomarkers
    - Assess clinical relevance
    - Synthesize knowledge across papers
    """
    
    def __init__(self, llm_client: LLMClient):
        """Uses GPT-4 via LLMClient"""
```

**Main Method - Analyze Single Citation:**
```python
def analyze_citation_context(
    self,
    citation_context: CitationContext,
    cited_paper: Publication,  # Original dataset paper
    citing_paper: Publication   # Paper using the dataset
) -> UsageAnalysis:
    """
    Analyzes how a citing paper uses the dataset.
    
    Inputs to GPT-4:
    - Cited paper title & abstract (dataset)
    - Citing paper title & abstract
    - Citation context (text around citation)
    
    GPT-4 Prompt Example:
    '''
    Analyze how this citing paper uses the dataset:
    
    Dataset: "Comprehensive RNA-seq of breast cancer tumors"
    Citing paper: "Machine learning predicts treatment response"
    Citation context: "We used the publicly available dataset [15] 
                      to train our ML model on 500 tumor samples..."
    
    Extract:
    1. Was dataset actually reused? (True/False)
    2. Usage type (validation, novel_application, comparison, etc.)
    3. Application domain (cancer genomics, drug discovery, etc.)
    4. Methodology used
    5. Key findings
    6. Novel biomarkers discovered
    7. Clinical relevance (high/medium/low/none)
    8. Validation status
    '''
    
    Returns:
        UsageAnalysis object with all extracted information
    """
```

**Output Structure (UsageAnalysis):**
```python
@dataclass
class UsageAnalysis:
    paper_id: str                    # DOI or PMID
    paper_title: str
    dataset_reused: bool             # ‚úÖ Was dataset actually used?
    usage_type: str                  # validation, novel_application, etc.
    confidence: float                # 0.0-1.0
    
    # Context
    research_question: str           # What did they study?
    application_domain: str          # cancer, diabetes, etc.
    methodology: str                 # machine learning, GWAS, etc.
    sample_info: str                 # How many samples used?
    
    # Results
    key_findings: List[str]          # Main discoveries
    novel_biomarkers: List[str]      # New biomarkers found
    
    # Clinical Translation
    clinical_relevance: str          # high, medium, low, none
    clinical_details: str            # Clinical trial info, etc.
    validation_status: str           # validated, in_progress, none
    
    # Explanation
    reasoning: str                   # Why GPT-4 classified it this way
```

**Batch Processing:**
```python
def analyze_batch(
    self,
    contexts: List[tuple],  # [(context, cited, citing), ...]
    batch_size: int = 5
) -> List[UsageAnalysis]:
    """
    Analyzes multiple citations efficiently.
    
    Processes papers in batches to optimize GPT-4 API calls.
    """
```

**Dataset Impact Synthesis:**
```python
def synthesize_dataset_impact(
    self,
    dataset_paper: Publication,
    usage_analyses: List[UsageAnalysis]
) -> DatasetImpactReport:
    """
    Synthesizes comprehensive impact report.
    
    Aggregates across all citing papers:
    - Total citations & reuse rate
    - Usage types breakdown
    - Application domains
    - All biomarkers discovered
    - Clinical translation status
    - Time span of impact
    - GPT-4 narrative summary
    
    Returns:
        Comprehensive DatasetImpactReport
    """
```

**Status:** ‚úÖ **FULLY IMPLEMENTED**

---

### File 5: Q&A System
**Location:** `omics_oracle_v2/lib/publications/analysis/qa_system.py`

**Purpose:** Interactive chat to ask questions about dataset usage

**This is EXACTLY what you described - "use chat agent to get answers"!**

**Key Class:**
```python
class DatasetQASystem:
    """
    Interactive Q&A system for dataset analysis.
    
    Allows users to ask natural language questions about how
    datasets are being used in scientific literature.
    """
```

**Main Method:**
```python
def ask(
    self,
    dataset: Publication,
    question: str,
    citation_analyses: List[UsageAnalysis],
    max_citations: int = 20
) -> Dict:
    """
    Ask a question about a dataset.
    
    Example Questions:
    - "What novel biomarkers were discovered?"
    - "How has this dataset been used in clinical research?"
    - "What are the most common applications?"
    - "Which findings have been validated?"
    
    Process:
    1. Build context from citation analyses
    2. Create GPT-4 prompt with context + question
    3. Generate evidence-based answer
    4. Extract supporting citations
    
    Returns:
        {
            "question": "What novel biomarkers...",
            "answer": "Three biomarkers were discovered: ...",
            "evidence": [
                {
                    "paper_title": "ML predicts response",
                    "relevance_score": 3,
                    "reasons": ["Discovered 5 biomarkers"],
                    "biomarkers": ["BRCA1", "TP53", ...]
                }
            ],
            "num_citations_analyzed": 50,
            "num_citations_used": 20
        }
    """
```

**Smart Question Suggestions:**
```python
def suggest_questions(
    self,
    dataset: Publication,
    citation_analyses: List[UsageAnalysis]
) -> List[str]:
    """
    Suggests relevant questions based on available data.
    
    Analyzes citation analyses to determine what questions
    would be most interesting/answerable.
    
    Example Output:
    [
        "How has this dataset been used in research?",
        "What novel biomarkers were discovered?",
        "What are the clinical applications?",
        "Which findings have been validated?",
        "What research domains have used this dataset?"
    ]
    """
```

**Status:** ‚úÖ **FULLY IMPLEMENTED**

---

## üîÑ Part 3: Integration with GEO Dataset Pipeline

### Where It Connects

**Starting Point: GEO Dataset Processing**
```python
# From SearchAgent ‚Üí DataAgent
processed_dataset = ProcessedDataset(
    geo_id="GSE123456",
    pubmed_ids=["12345678", "87654321"],  # ‚Üê Connection point!
    quality_score=0.85,
    # ... other metadata
)
```

**Connection to Citation Pipeline:**
```python
# File: omics_oracle_v2/lib/publications/pipeline.py
class PublicationPipeline:
    
    def search_with_citations(
        self,
        query: str,
        enable_citation_analysis: bool = True,
        enable_pdf_download: bool = True,
        enable_llm_analysis: bool = True
    ):
        """
        Complete workflow including citations.
        
        Steps:
        1. Search publications (from GEO dataset pubmed_ids)
        2. Enrich with Semantic Scholar citations
        3. Get citing papers (CitationAnalyzer)
        4. Download PDFs (PDFDownloader)
        5. Extract full text (FullTextExtractor)
        6. Analyze with LLM (LLMCitationAnalyzer)
        7. Enable Q&A (DatasetQASystem)
        """
```

**Complete Flow Example:**
```python
# Example usage connecting GEO dataset to citation analysis

# Step 1: Get GEO dataset (existing SearchAgent workflow)
dataset = search_agent.search("breast cancer RNA-seq")
# Result: GSE123456 with pubmed_id="12345678"

# Step 2: Fetch dataset publication
pipeline = PublicationPipeline()
dataset_publication = pipeline.fetch_by_pmid("12345678")

# Step 3: Get papers citing this dataset
citation_analyzer = CitationAnalyzer(scholar_client)
citing_papers = citation_analyzer.get_citing_papers(
    dataset_publication,
    max_results=100
)
# Result: 87 papers citing this dataset

# Step 4: Download PDFs
pdf_downloader = PDFDownloader(download_dir="./data/pdfs")
downloaded = pdf_downloader.download_batch(citing_papers)
# Result: 61/87 PDFs downloaded (70% success)

# Step 5: Extract full text
extractor = FullTextExtractor()
full_texts = {}
for paper, pdf_path in downloaded.items():
    text = extractor.extract_text(pdf_path)
    full_texts[paper.pmid] = text
# Result: 61 full-text documents

# Step 6: LLM Analysis - How is dataset used?
llm_analyzer = LLMCitationAnalyzer(llm_client)

# Build citation contexts
contexts = []
for citing_paper in citing_papers:
    citation_contexts = citation_analyzer.get_citation_contexts(
        dataset_publication,
        citing_paper
    )
    for ctx in citation_contexts:
        contexts.append((ctx, dataset_publication, citing_paper))

# Analyze with GPT-4
usage_analyses = llm_analyzer.analyze_batch(
    contexts,
    batch_size=5
)
# Result: 87 UsageAnalysis objects

# Count dataset reuse
reused = sum(1 for a in usage_analyses if a.dataset_reused)
print(f"Dataset reused in {reused}/87 papers ({reused/87*100:.1f}%)")
# Output: "Dataset reused in 34/87 papers (39.1%)"

# Extract biomarkers
all_biomarkers = []
for analysis in usage_analyses:
    all_biomarkers.extend(analysis.novel_biomarkers)
print(f"Novel biomarkers discovered: {len(set(all_biomarkers))}")
# Output: "Novel biomarkers discovered: 23"

# Step 7: Interactive Q&A
qa_system = DatasetQASystem(llm_client)

# Ask questions
answer1 = qa_system.ask(
    dataset_publication,
    "What novel biomarkers were discovered using this dataset?",
    usage_analyses
)
print(answer1["answer"])
# Output: "Three novel biomarkers were consistently identified 
#          across multiple studies: BRCA1 (mentioned in 8 papers), 
#          TP53 (mentioned in 6 papers), and ESR1 (mentioned in 5 papers)..."

answer2 = qa_system.ask(
    dataset_publication,
    "What are the clinical applications of research using this dataset?",
    usage_analyses
)
print(answer2["answer"])
# Output: "The dataset has been used in 12 clinical studies, primarily 
#          for treatment response prediction (7 papers) and risk 
#          stratification (5 papers)..."

# Step 8: Generate comprehensive impact report
impact_report = llm_analyzer.synthesize_dataset_impact(
    dataset_publication,
    usage_analyses
)

print(f"""
Dataset Impact Report:
- Total citations: {impact_report.total_citations}
- Dataset reused: {impact_report.dataset_reuse_count} papers
- Reuse rate: {impact_report.dataset_reuse_count/impact_report.total_citations*100:.1f}%
- Time span: {impact_report.time_span_years} years
- Usage types: {impact_report.usage_types}
- Novel biomarkers: {len(impact_report.novel_biomarkers)}
- Clinical trials initiated: {impact_report.clinical_translation.trials_initiated}

Summary:
{impact_report.summary}
""")
```

**Status:** ‚úÖ **ALL INTEGRATED AND WORKING**

---

