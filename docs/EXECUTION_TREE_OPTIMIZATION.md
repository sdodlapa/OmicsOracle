# OmicsOracle Complete Execution Tree - DFS/BFS Analysis

**Date:** October 16, 2025  
**Purpose:** Depth-First tracing and Breadth-First visualization for optimization

---

## Tree Structure - Complete Execution Graph

```
ROOT: dashboard_v2.html (User Action: Search)
â”‚
â”œâ”€[USER INTERACTION TREE]
â”‚   â”‚
â”‚   â”œâ”€ performSearch()
â”‚   â”‚   â”œâ”€ Validate input
â”‚   â”‚   â”œâ”€ Build request payload
â”‚   â”‚   â”œâ”€ HTTP POST /api/search
â”‚   â”‚   â””â”€ Wait for response
â”‚   â”‚
â”‚   â”œâ”€ discoverCitationsForDataset(index)
â”‚   â”‚   â”œâ”€ Get dataset from results[index]
â”‚   â”‚   â”œâ”€ HTTP POST /api/datasets/{geo_id}/discover-citations
â”‚   â”‚   â””â”€ Refresh search results
â”‚   â”‚
â”‚   â””â”€ downloadPapersForDataset(index)
â”‚       â”œâ”€ Get dataset from results[index]
â”‚       â”œâ”€ HTTP POST /api/enrich-fulltext
â”‚       â””â”€ Update UI with PDFs
â”‚
â”œâ”€[API GATEWAY TREE - POST /api/search]
â”‚   â”‚
â”‚   â”œâ”€ main.py
â”‚   â”‚   â”œâ”€ FastAPI app initialization
â”‚   â”‚   â”œâ”€ Middleware chain
â”‚   â”‚   â”‚   â”œâ”€ CORS middleware
â”‚   â”‚   â”‚   â”œâ”€ Rate limit middleware
â”‚   â”‚   â”‚   â”œâ”€ Request logging middleware
â”‚   â”‚   â”‚   â””â”€ Error handling middleware
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€ Route to agents_router
â”‚   â”‚
â”‚   â””â”€ routes/agents.py::execute_search()
â”‚       â”œâ”€ Parse SearchRequest (Pydantic)
â”‚       â”œâ”€ Call SearchService.execute_search()
â”‚       â”œâ”€ Format SearchResponse
â”‚       â””â”€ Return HTTP 200 + JSON
â”‚
â”œâ”€[SERVICE LAYER TREE]
â”‚   â”‚
â”‚   â””â”€ services/search_service.py::SearchService
â”‚       â”‚
â”‚       â”œâ”€ execute_search(request)
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€ _build_search_config()
â”‚       â”‚   â”‚   â””â”€ Return OrchestratorConfig
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€ _build_query()
â”‚       â”‚   â”‚   â”œâ”€ Parse search_terms
â”‚       â”‚   â”‚   â”œâ”€ Apply filters (organism, study_type)
â”‚       â”‚   â”‚   â””â”€ Return optimized query string
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€ SearchOrchestrator.search()  [ASYNC CALL]
â”‚       â”‚   â”‚   â””â”€ [BRANCHES TO ORCHESTRATOR TREE]
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€ _rank_datasets()
â”‚       â”‚   â”‚   â”œâ”€ Calculate relevance_score
â”‚       â”‚   â”‚   â”œâ”€ Generate match_reasons
â”‚       â”‚   â”‚   â””â”€ Sort by score
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€ _build_dataset_responses()  [ASYNC CALL]
â”‚       â”‚       â””â”€ [BRANCHES TO ENRICHMENT TREE]
â”‚       â”‚
â”‚       â””â”€ Properties
â”‚           â””â”€ geo_cache (lazy-loaded)
â”‚               â”œâ”€ Import UnifiedDatabase
â”‚               â”œâ”€ Import create_geo_cache
â”‚               â””â”€ Initialize GEOCache instance
â”‚
â”œâ”€[ORCHESTRATOR TREE - Parallel Execution]
â”‚   â”‚
â”‚   â””â”€ search_orchestration/orchestrator.py::SearchOrchestrator
â”‚       â”‚
â”‚       â””â”€ search(query, max_geo_results, max_publication_results)
â”‚           â”‚
â”‚           â”œâ”€ _detect_query_type(query)
â”‚           â”‚   â”œâ”€ Check if GEO ID pattern (GSE\d+)
â”‚           â”‚   â”œâ”€ Check if PMID pattern
â”‚           â”‚   â””â”€ Default to keyword search
â”‚           â”‚
â”‚           â”œâ”€ _optimize_query(query)  [IF semantic=True]
â”‚           â”‚   â”œâ”€ QueryOptimizer.optimize()
â”‚           â”‚   â”‚   â”œâ”€ NER extraction (BiomedicalNER)
â”‚           â”‚   â”‚   â”‚   â”œâ”€ Extract diseases
â”‚           â”‚   â”‚   â”‚   â”œâ”€ Extract genes
â”‚           â”‚   â”‚   â”‚   â””â”€ Extract organisms
â”‚           â”‚   â”‚   â”‚
â”‚           â”‚   â”‚   â”œâ”€ SynonymExpander.expand()
â”‚           â”‚   â”‚   â”‚   â”œâ”€ Query SapBERT embeddings
â”‚           â”‚   â”‚   â”‚   â””â”€ Add semantic synonyms
â”‚           â”‚   â”‚   â”‚
â”‚           â”‚   â”‚   â””â”€ Return optimized query
â”‚           â”‚   â”‚
â”‚           â”‚   â””â”€ Update query string
â”‚           â”‚
â”‚           â”œâ”€ Cache Check
â”‚           â”‚   â”œâ”€ RedisCache.get_search_result(query_hash)
â”‚           â”‚   â”œâ”€ IF HIT: Return cached SearchResult (ENDS HERE)
â”‚           â”‚   â””â”€ IF MISS: Continue to parallel search
â”‚           â”‚
â”‚           â”œâ”€ PARALLEL SEARCH (asyncio.gather)
â”‚           â”‚   â”‚
â”‚           â”‚   â”œâ”€[BRANCH 1: GEO Search Thread]
â”‚           â”‚   â”‚   â””â”€ _search_geo(query)
â”‚           â”‚   â”‚       â”œâ”€ GEOClient.search(query)
â”‚           â”‚   â”‚       â”‚   â”œâ”€ NCBIClient.esearch(db="gds", term=query)
â”‚           â”‚   â”‚       â”‚   â”‚   â”œâ”€ HTTP GET to eutils.ncbi.nlm.nih.gov
â”‚           â”‚   â”‚       â”‚   â”‚   â”œâ”€ Parse XML response
â”‚           â”‚   â”‚       â”‚   â”‚   â””â”€ Return GEO IDs list
â”‚           â”‚   â”‚       â”‚   â”‚
â”‚           â”‚   â”‚       â”‚   â”œâ”€ GEOClient.batch_get_metadata(geo_ids)
â”‚           â”‚   â”‚       â”‚   â”‚   â”œâ”€ For each geo_id:
â”‚           â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€ NCBIClient.esummary(db="gds", id=geo_id)
â”‚           â”‚   â”‚       â”‚   â”‚   â”‚   â”œâ”€ Parse GEO metadata
â”‚           â”‚   â”‚       â”‚   â”‚   â”‚   â””â”€ Create GEOSeriesMetadata object
â”‚           â”‚   â”‚       â”‚   â”‚   â””â”€ Return metadata dict
â”‚           â”‚   â”‚       â”‚   â”‚
â”‚           â”‚   â”‚       â”‚   â””â”€ Return SearchResult
â”‚           â”‚   â”‚       â”‚
â”‚           â”‚   â”‚       â””â”€ Store in UnifiedDB
â”‚           â”‚   â”‚           â””â”€ PipelineCoordinator.save_search_result()
â”‚           â”‚   â”‚
â”‚           â”‚   â”œâ”€[BRANCH 2: PubMed Search Thread]
â”‚           â”‚   â”‚   â””â”€ _search_publications_pubmed(query)
â”‚           â”‚   â”‚       â””â”€ PubMedClient.search(query)
â”‚           â”‚   â”‚           â”œâ”€ NCBIClient.esearch(db="pubmed", term=query)
â”‚           â”‚   â”‚           â”‚   â”œâ”€ HTTP GET to eutils.ncbi.nlm.nih.gov
â”‚           â”‚   â”‚           â”‚   â””â”€ Return PMID list
â”‚           â”‚   â”‚           â”‚
â”‚           â”‚   â”‚           â”œâ”€ PubMedClient.fetch_details(pmids)
â”‚           â”‚   â”‚           â”‚   â”œâ”€ NCBIClient.efetch(db="pubmed", ids=pmids)
â”‚           â”‚   â”‚           â”‚   â”œâ”€ Parse XML for each paper
â”‚           â”‚   â”‚           â”‚   â””â”€ Create Publication objects
â”‚           â”‚   â”‚           â”‚
â”‚           â”‚   â”‚           â””â”€ Return List[Publication]
â”‚           â”‚   â”‚
â”‚           â”‚   â””â”€[BRANCH 3: OpenAlex Search Thread]
â”‚           â”‚       â””â”€ _search_publications_openalex(query)
â”‚           â”‚           â””â”€ OpenAlexClient.search_works(query)
â”‚           â”‚               â”œâ”€ HTTP GET to api.openalex.org/works
â”‚           â”‚               â”œâ”€ Parse JSON response
â”‚           â”‚               â””â”€ Return List[Publication]
â”‚           â”‚
â”‚           â”œâ”€ Merge Results
â”‚           â”‚   â”œâ”€ Combine geo_datasets
â”‚           â”‚   â”œâ”€ Deduplicate publications (by PMID/DOI)
â”‚           â”‚   â””â”€ Create unified SearchResult
â”‚           â”‚
â”‚           â”œâ”€ Cache Result
â”‚           â”‚   â””â”€ RedisCache.set_search_result(query_hash, result, ttl=3600)
â”‚           â”‚
â”‚           â””â”€ Return SearchResult
â”‚
â”œâ”€[ENRICHMENT TREE - Dataset Response Building]
â”‚   â”‚
â”‚   â””â”€ services/search_service.py::_build_dataset_responses()
â”‚       â”‚
â”‚       â””â”€ For each ranked_dataset:
â”‚           â”‚
â”‚           â”œâ”€ geo_cache.get(geo_id)  [ASYNC CALL]
â”‚           â”‚   â””â”€ [BRANCHES TO GEOCACHE TREE]
â”‚           â”‚
â”‚           â”œâ”€ Extract metrics from geo_data
â”‚           â”‚   â”œâ”€ papers = geo_data["papers"]["original"]
â”‚           â”‚   â”œâ”€ citation_count = len(papers)
â”‚           â”‚   â”œâ”€ pdf_count = count(download_history=="downloaded")
â”‚           â”‚   â”œâ”€ processed_count = count(extraction != None)
â”‚           â”‚   â””â”€ completion_rate = (pdf_count / citation_count) * 100
â”‚           â”‚
â”‚           â””â”€ Create DatasetResponse
â”‚               â”œâ”€ geo_id
â”‚               â”œâ”€ title, summary, organism
â”‚               â”œâ”€ relevance_score, match_reasons
â”‚               â”œâ”€ citation_count  [ENRICHED FROM DB]
â”‚               â”œâ”€ pdf_count       [ENRICHED FROM DB]
â”‚               â””â”€ completion_rate [ENRICHED FROM DB]
â”‚
â”œâ”€[GEOCACHE TREE - 2-Tier Cache with Auto-Discovery]
â”‚   â”‚
â”‚   â””â”€ storage/registry/geo_cache.py::GEOCache
â”‚       â”‚
â”‚       â””â”€ get(geo_id)  [ASYNC METHOD]
â”‚           â”‚
â”‚           â”œâ”€[TIER 1: Redis Hot Cache]
â”‚           â”‚   â”œâ”€ RedisCache.get_geo_metadata(geo_id)
â”‚           â”‚   â”‚   â”œâ”€ redis.get(f"geo_complete:{geo_id}")
â”‚           â”‚   â”‚   â””â”€ IF HIT: Return data (ENDS HERE - 0.2ms)
â”‚           â”‚   â”‚
â”‚           â”‚   â””â”€ Check memory_fallback dict
â”‚           â”‚       â””â”€ IF HIT: Return data (ENDS HERE - <0.1ms)
â”‚           â”‚
â”‚           â”œâ”€[TIER 2: UnifiedDB Warm Cache]
â”‚           â”‚   â”œâ”€ UnifiedDatabase.get_complete_geo_data(geo_id)
â”‚           â”‚   â”‚   â”œâ”€ Query geo_datasets table
â”‚           â”‚   â”‚   â”œâ”€ Query universal_identifiers (JOIN)
â”‚           â”‚   â”‚   â”œâ”€ Query url_discovery (LEFT JOIN)
â”‚           â”‚   â”‚   â”œâ”€ Query pdf_acquisition (LEFT JOIN)
â”‚           â”‚   â”‚   â”œâ”€ Query content_extraction (LEFT JOIN)
â”‚           â”‚   â”‚   â”‚
â”‚           â”‚   â”‚   â””â”€ Build response dict:
â”‚           â”‚   â”‚       â”œâ”€ geo: {metadata}
â”‚           â”‚   â”‚       â””â”€ papers: {original: [], citing: []}
â”‚           â”‚   â”‚
â”‚           â”‚   â”œâ”€ IF HIT: 
â”‚           â”‚   â”‚   â”œâ”€ Promote to Redis (_promote_to_hot_tier)
â”‚           â”‚   â”‚   â””â”€ Return data (ENDS HERE - 50ms)
â”‚           â”‚   â”‚
â”‚           â”‚   â””â”€ IF MISS: Continue to auto-discovery
â”‚           â”‚
â”‚           â””â”€[TIER 3: Auto-Discovery (NEW!)]
â”‚               â””â”€ _auto_discover_and_populate(geo_id)  [ASYNC METHOD]
â”‚                   â””â”€ [BRANCHES TO AUTO-DISCOVERY TREE]
â”‚
â”œâ”€[AUTO-DISCOVERY TREE - Citation Discovery Pipeline]
â”‚   â”‚
â”‚   â””â”€ storage/registry/geo_cache.py::_auto_discover_and_populate()
â”‚       â”‚
â”‚       â”œâ”€[STEP 1: Fetch GEO Metadata]
â”‚       â”‚   â””â”€ GEOClient.get_metadata(geo_id)
â”‚       â”‚       â”œâ”€ NCBIClient.esummary(db="gds", id=geo_id)
â”‚       â”‚       â”‚   â”œâ”€ HTTP GET to eutils.ncbi.nlm.nih.gov
â”‚       â”‚       â”‚   â”œâ”€ Parse XML response
â”‚       â”‚       â”‚   â””â”€ Extract: title, summary, organism, platforms, pubmed_ids
â”‚       â”‚       â”‚
â”‚       â”‚       â””â”€ Return GEOSeriesMetadata
â”‚       â”‚           â”œâ”€ geo_id: "GSE189158"
â”‚       â”‚           â”œâ”€ title: "NOMe-HiC: joint profiling..."
â”‚       â”‚           â”œâ”€ organism: "Homo sapiens"
â”‚       â”‚           â”œâ”€ pubmed_ids: ["36927507"]
â”‚       â”‚           â””â”€ sample_count, platforms, etc.
â”‚       â”‚
â”‚       â”œâ”€[STEP 2: Citation Discovery]
â”‚       â”‚   â””â”€ GEOCitationDiscovery.find_citing_papers(metadata, max_results=100)
â”‚       â”‚       â””â”€ [BRANCHES TO CITATION DISCOVERY TREE]
â”‚       â”‚
â”‚       â”œâ”€[STEP 3: Store in Database]
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€ Store GEO Dataset
â”‚       â”‚   â”‚   â””â”€ UnifiedDatabase.insert_geo_dataset(GEODataset)
â”‚       â”‚   â”‚       â”œâ”€ INSERT INTO geo_datasets
â”‚       â”‚   â”‚       â””â”€ ON CONFLICT UPDATE (upsert)
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€ Store Citations (for each paper)
â”‚       â”‚       â””â”€ UnifiedDatabase.insert_universal_identifier(UniversalIdentifier)
â”‚       â”‚           â”œâ”€ INSERT INTO universal_identifiers
â”‚       â”‚           â”‚   (geo_id, pmid, doi, title, authors, journal, pub_date)
â”‚       â”‚           â””â”€ ON CONFLICT UPDATE (upsert)
â”‚       â”‚
â”‚       â”œâ”€[STEP 4: Retrieve Complete Data]
â”‚       â”‚   â””â”€ UnifiedDatabase.get_complete_geo_data(geo_id)
â”‚       â”‚       â””â”€ Return enriched data with citations
â”‚       â”‚
â”‚       â””â”€ Return geo_data (or None if failed)
â”‚
â”œâ”€[CITATION DISCOVERY TREE - Multi-Source Parallel Discovery]
â”‚   â”‚
â”‚   â””â”€ citation_discovery/geo_discovery.py::GEOCitationDiscovery
â”‚       â”‚
â”‚       â””â”€ find_citing_papers(geo_metadata, max_results)
â”‚           â”‚
â”‚           â”œâ”€ Extract original PMID
â”‚           â”‚   â””â”€ original_pmid = geo_metadata.pubmed_ids[0]
â”‚           â”‚
â”‚           â”œâ”€[STRATEGY A: Citation-Based Discovery]
â”‚           â”‚   â””â”€ _find_via_citation(original_pmid)
â”‚           â”‚       â”‚
â”‚           â”‚       â”œâ”€ Check Cache
â”‚           â”‚       â”‚   â””â”€ DiscoveryCache.get(f"citation_{pmid}")
â”‚           â”‚       â”‚       â””â”€ IF HIT: Return cached (ENDS HERE)
â”‚           â”‚       â”‚
â”‚           â”‚       â””â”€ PARALLEL CITATION SOURCES (ThreadPoolExecutor)
â”‚           â”‚           â”‚
â”‚           â”‚           â”œâ”€[THREAD 1: OpenAlex]
â”‚           â”‚           â”‚   â””â”€ fetch_openalex()
â”‚           â”‚           â”‚       â”œâ”€ OpenAlexClient.get_work(f"pmid:{pmid}")
â”‚           â”‚           â”‚       â”‚   â”œâ”€ HTTP GET /works/pmid:{pmid}
â”‚           â”‚           â”‚       â”‚   â””â”€ Get work_id
â”‚           â”‚           â”‚       â”‚
â”‚           â”‚           â”‚       â”œâ”€ OpenAlexClient.get_citations(work_id)
â”‚           â”‚           â”‚       â”‚   â”œâ”€ HTTP GET /works?filter=cites:{work_id}
â”‚           â”‚           â”‚       â”‚   â”œâ”€ Parse JSON (up to 50 results)
â”‚           â”‚           â”‚       â”‚   â””â”€ Extract: pmid, doi, title, authors, etc.
â”‚           â”‚           â”‚       â”‚
â”‚           â”‚           â”‚       â””â”€ Return List[Publication]
â”‚           â”‚           â”‚
â”‚           â”‚           â”œâ”€[THREAD 2: Semantic Scholar]
â”‚           â”‚           â”‚   â””â”€ fetch_semantic_scholar()
â”‚           â”‚           â”‚       â”œâ”€ SemanticScholarClient.get_paper(f"PMID:{pmid}")
â”‚           â”‚           â”‚       â”‚   â”œâ”€ HTTP GET /paper/PMID:{pmid}
â”‚           â”‚           â”‚       â”‚   â””â”€ Get paper_id
â”‚           â”‚           â”‚       â”‚
â”‚           â”‚           â”‚       â”œâ”€ SemanticScholarClient.get_citations(paper_id)
â”‚           â”‚           â”‚       â”‚   â”œâ”€ HTTP GET /paper/{paper_id}/citations
â”‚           â”‚           â”‚       â”‚   â”œâ”€ Parse JSON (up to 100 results)
â”‚           â”‚           â”‚       â”‚   â””â”€ Extract metadata
â”‚           â”‚           â”‚       â”‚
â”‚           â”‚           â”‚       â””â”€ Return List[Publication]
â”‚           â”‚           â”‚
â”‚           â”‚           â”œâ”€[THREAD 3: Europe PMC]
â”‚           â”‚           â”‚   â””â”€ fetch_europepmc()
â”‚           â”‚           â”‚       â””â”€ EuropePMCClient.get_citations(pmid)
â”‚           â”‚           â”‚           â”œâ”€ HTTP GET /europepmc/webservices/rest/MED/{pmid}/citations
â”‚           â”‚           â”‚           â”œâ”€ Parse XML response
â”‚           â”‚           â”‚           â””â”€ Return List[Publication]
â”‚           â”‚           â”‚
â”‚           â”‚           â”œâ”€[THREAD 4: PubMed Citations]
â”‚           â”‚           â”‚   â””â”€ fetch_pubmed_citations()
â”‚           â”‚           â”‚       â””â”€ PubMedClient.get_citing_papers(pmid)
â”‚           â”‚           â”‚           â”œâ”€ HTTP GET eutils (elink)
â”‚           â”‚           â”‚           â”œâ”€ Get linked PMIDs
â”‚           â”‚           â”‚           â””â”€ Return List[Publication]
â”‚           â”‚           â”‚
â”‚           â”‚           â””â”€ Wait for all threads (futures.as_completed)
â”‚           â”‚               â”œâ”€ Merge results from all sources
â”‚           â”‚               â”œâ”€ Deduplicate by PMID/DOI
â”‚           â”‚               â””â”€ Return combined list
â”‚           â”‚
â”‚           â”œâ”€[STRATEGY B: Mention-Based Discovery]
â”‚           â”‚   â””â”€ _find_via_geo_mention(geo_id)
â”‚           â”‚       â”‚
â”‚           â”‚       â”œâ”€ Check Cache
â”‚           â”‚       â”‚   â””â”€ DiscoveryCache.get(f"mention_{geo_id}")
â”‚           â”‚       â”‚
â”‚           â”‚       â””â”€ PubMed Search
â”‚           â”‚           â””â”€ PubMedClient.search(f'"{geo_id}"[All Fields]')
â”‚           â”‚               â”œâ”€ HTTP GET esearch
â”‚           â”‚               â”œâ”€ Get PMIDs mentioning GEO ID
â”‚           â”‚               â”œâ”€ Fetch metadata for each PMID
â”‚           â”‚               â””â”€ Return List[Publication]
â”‚           â”‚
â”‚           â”œâ”€ Merge Results
â”‚           â”‚   â”œâ”€ Combine Strategy A + Strategy B
â”‚           â”‚   â”œâ”€ Deduplicate by PMID
â”‚           â”‚   â”œâ”€ Filter by quality (if enabled)
â”‚           â”‚   â””â”€ Limit to max_results
â”‚           â”‚
â”‚           â”œâ”€ Cache Results
â”‚           â”‚   â””â”€ DiscoveryCache.set(cache_key, results, ttl=604800)
â”‚           â”‚
â”‚           â””â”€ Return CitationDiscoveryResult
â”‚               â”œâ”€ geo_id
â”‚               â”œâ”€ original_pmid
â”‚               â”œâ”€ citing_papers: List[Publication]
â”‚               â”œâ”€ total_found: int
â”‚               â””â”€ sources_used: List[str]
â”‚
â”œâ”€[EXTERNAL API CLIENT TREE - HTTP Request Layer]
â”‚   â”‚
â”‚   â”œâ”€ NCBIClient (E-utilities)
â”‚   â”‚   â”œâ”€ esearch(db, term) â†’ HTTP GET
â”‚   â”‚   â”œâ”€ efetch(db, ids) â†’ HTTP GET
â”‚   â”‚   â”œâ”€ esummary(db, ids) â†’ HTTP GET
â”‚   â”‚   â””â”€ Rate limiting: 3 req/s (10 with API key)
â”‚   â”‚
â”‚   â”œâ”€ OpenAlexClient
â”‚   â”‚   â”œâ”€ get_work(id) â†’ HTTP GET /works/{id}
â”‚   â”‚   â”œâ”€ get_citations(id) â†’ HTTP GET /works?filter=cites:{id}
â”‚   â”‚   â”œâ”€ search_works(query) â†’ HTTP GET /works?search={query}
â”‚   â”‚   â””â”€ Rate limiting: 10 req/s (polite pool)
â”‚   â”‚
â”‚   â”œâ”€ PubMedClient
â”‚   â”‚   â”œâ”€ search(query) â†’ esearch + efetch
â”‚   â”‚   â”œâ”€ fetch_details(pmids) â†’ efetch
â”‚   â”‚   â””â”€ Rate limiting: 3 req/s
â”‚   â”‚
â”‚   â”œâ”€ SemanticScholarClient
â”‚   â”‚   â”œâ”€ get_paper(id) â†’ HTTP GET /paper/{id}
â”‚   â”‚   â”œâ”€ get_citations(id) â†’ HTTP GET /paper/{id}/citations
â”‚   â”‚   â””â”€ Rate limiting: 1 req/s (5 with API key)
â”‚   â”‚
â”‚   â””â”€ EuropePMCClient
â”‚       â”œâ”€ search(query) â†’ HTTP GET /search
â”‚       â”œâ”€ get_citations(pmid) â†’ HTTP GET /citations/{pmid}
â”‚       â””â”€ Rate limiting: Best effort
â”‚
â””â”€[EXTERNAL DATA SOURCES - API Endpoints]
    â”‚
    â”œâ”€ NCBI E-utilities
    â”‚   â”œâ”€ https://eutils.ncbi.nlm.nih.gov/entrez/eutils/
    â”‚   â”œâ”€ Databases: gds, pubmed, sra
    â”‚   â””â”€ Response: XML
    â”‚
    â”œâ”€ OpenAlex API
    â”‚   â”œâ”€ https://api.openalex.org/
    â”‚   â””â”€ Response: JSON
    â”‚
    â”œâ”€ PubMed Central
    â”‚   â”œâ”€ https://eutils.ncbi.nlm.nih.gov/
    â”‚   â””â”€ Response: XML
    â”‚
    â”œâ”€ Semantic Scholar API
    â”‚   â”œâ”€ https://api.semanticscholar.org/
    â”‚   â””â”€ Response: JSON
    â”‚
    â””â”€ Europe PMC API
        â”œâ”€ https://www.ebi.ac.uk/europepmc/
        â””â”€ Response: XML/JSON
```

---

## Depth-First Search (DFS) - Critical Paths

### Path 1: Fast Path (Cached Search)
```
DFS Path: User â†’ Result (Cached)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Depth 0: dashboard_v2.html::performSearch()
    â†“
Depth 1: POST /api/search
    â†“
Depth 2: routes/agents.py::execute_search()
    â†“
Depth 3: search_service.py::execute_search()
    â†“
Depth 4: orchestrator.py::search()
    â†“
Depth 5: redis_cache.py::get_search_result()
    â†“ CACHE HIT
Depth 6: Return cached SearchResult
    â†“
Depth 5: orchestrator returns
    â†“
Depth 4: search_service enrichment (skipped)
    â†“
Depth 3: Format SearchResponse
    â†“
Depth 2: HTTP 200 + JSON
    â†“
Depth 1: dashboard receives
    â†“
Depth 0: displayResults()

Total Depth: 6 levels
Total Time: ~100ms
Bottleneck: Network latency (HTTP round-trip)
Optimization: âœ… Optimal (Redis cache hit)
```

### Path 2: Slow Path (First Search + Auto-Discovery)
```
DFS Path: User â†’ Auto-Discovery â†’ Result
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Depth 0: dashboard_v2.html::performSearch()
    â†“
Depth 1: POST /api/search
    â†“
Depth 2: routes/agents.py::execute_search()
    â†“
Depth 3: search_service.py::execute_search()
    â†“
Depth 4: orchestrator.py::search()
    â†“
Depth 5: redis_cache.py::get_search_result() â†’ MISS
    â†“
Depth 6: PARALLEL EXECUTION (3 threads)
    â”œâ”€ Thread 1: geo_client.py::search()
    â”‚   â†“
    â”‚   Depth 7: ncbi_client.py::esearch(db="gds")
    â”‚       â†“
    â”‚       Depth 8: HTTP GET to eutils.ncbi.nlm.nih.gov
    â”‚           â†“
    â”‚           Depth 9: NCBI GEO Database (EXTERNAL)
    â”‚
    â”œâ”€ Thread 2: pubmed_client.py::search()
    â”‚   â””â”€ (Similar depth to Thread 1)
    â”‚
    â””â”€ Thread 3: openalex_client.py::search_works()
        â””â”€ (Similar depth to Thread 1)
    â†“
Depth 6: Merge results
    â†“
Depth 5: search_service.py::_build_dataset_responses()
    â†“
Depth 6: geo_cache.py::get(geo_id)
    â†“
Depth 7: redis_cache.py::get_geo_metadata() â†’ MISS
    â†“
Depth 8: unified_db.py::get_complete_geo_data() â†’ MISS
    â†“
Depth 9: geo_cache.py::_auto_discover_and_populate()
    â†“
    Depth 10: geo_client.py::get_metadata()
        â†“
        Depth 11: ncbi_client.py::esummary()
            â†“
            Depth 12: HTTP GET to NCBI
                â†“
                Depth 13: NCBI GEO Database (EXTERNAL)
    â†“
    Depth 10: geo_discovery.py::find_citing_papers()
        â†“
        Depth 11: PARALLEL THREADS (4 citation sources)
            â”œâ”€ Thread A: openalex_client.py
            â”‚   â†“
            â”‚   Depth 12: HTTP GET to api.openalex.org
            â”‚       â†“
            â”‚       Depth 13: OpenAlex API (EXTERNAL)
            â”‚           â†“
            â”‚           Depth 14: OpenAlex Database
            â”‚
            â”œâ”€ Thread B: semantic_scholar_client.py
            â”‚   â””â”€ Similar depth
            â”‚
            â”œâ”€ Thread C: europepmc_client.py
            â”‚   â””â”€ Similar depth
            â”‚
            â””â”€ Thread D: pubmed_client.py
                â””â”€ Similar depth
        â†“
        Depth 11: Merge & deduplicate citations
    â†“
    Depth 10: unified_db.py::insert_geo_dataset()
        â†“
        Depth 11: SQLite INSERT
    â†“
    Depth 10: For each citation:
        â””â”€ unified_db.py::insert_universal_identifier()
            â†“
            Depth 11: SQLite INSERT
    â†“
    Depth 10: unified_db.py::get_complete_geo_data()
        â†“
        Depth 11: SQLite SELECT with JOINs
    â†“
Depth 9: Return enriched geo_data
    â†“
Depth 8: geo_cache promotes to Redis
    â†“
Depth 7: Return to search_service
    â†“
Depth 6: Build DatasetResponse with enriched metrics
    â†“
Depth 5: Format SearchResponse
    â†“
Depth 4: Return to routes
    â†“
Depth 3: HTTP 200 + JSON
    â†“
Depth 2: dashboard receives
    â†“
Depth 1: displayResults()
    â†“
Depth 0: User sees results

Total Depth: 14 levels (deepest path)
Total Time: 5-30 seconds
Bottleneck: Citation discovery parallel threads (Depth 11-14)
Optimization Opportunities: ðŸŽ¯ See optimization section below
```

### Path 3: Manual Discovery Button Click
```
DFS Path: User Click â†’ Discovery â†’ Update
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Depth 0: dashboard_v2.html::discoverCitationsForDataset(index)
    â†“
Depth 1: POST /api/datasets/{geo_id}/discover-citations
    â†“
Depth 2: routes/agents.py::discover_citations()
    â†“
Depth 3: geo_discovery.py::find_citing_papers()
    â†“
Depth 4: [Same as auto-discovery from Depth 11 onwards]
    â†“
... (citation discovery tree)
    â†“
Depth 2: Return {citations_found, success}
    â†“
Depth 1: dashboard receives response
    â†“
Depth 0: Alert user + refresh search

Total Depth: 13 levels
Total Time: 8-25 seconds
Note: Now redundant with auto-discovery!
Optimization: âš ï¸ Can be removed (auto-discovery handles this)
```

---

## Breadth-First Search (BFS) - Level-by-Level Analysis

### BFS Level Map

```
LEVEL 0 (Frontend)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 1
â”œâ”€ dashboard_v2.html
â”‚   â”œâ”€ performSearch()
â”‚   â”œâ”€ discoverCitationsForDataset()
â”‚   â””â”€ downloadPapersForDataset()

Connections: 3 user actions
Latency: <1ms (client-side JS)
Parallelization: None
Bottleneck: User input
Optimization: âœ… Already minimal


LEVEL 1 (HTTP Layer)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 3
â”œâ”€ HTTP POST /api/search
â”œâ”€ HTTP POST /api/datasets/{geo_id}/discover-citations
â””â”€ HTTP POST /api/enrich-fulltext

Connections: Network â†’ API Gateway
Latency: 5-20ms (network round-trip)
Parallelization: None (sequential user actions)
Bottleneck: Network latency
Optimization: âœ… Can't optimize (network-bound)


LEVEL 2 (API Gateway)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 4
â”œâ”€ main.py (FastAPI app)
â”œâ”€ routes/agents.py::execute_search()
â”œâ”€ routes/agents.py::discover_citations()
â””â”€ routes/agents.py::enrich_fulltext()

Connections: Routes â†’ Services
Latency: <5ms (routing + middleware)
Parallelization: None (request handling)
Bottleneck: Middleware chain
Optimization: 
  âš ï¸ Consider:
    - Remove unnecessary middleware for read-only endpoints
    - Cache CORS headers
    - Optimize Pydantic validation


LEVEL 3 (Service Layer)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 2
â”œâ”€ search_service.py::SearchService
â”‚   â”œâ”€ execute_search()
â”‚   â”œâ”€ _build_dataset_responses()
â”‚   â””â”€ _rank_datasets()
â”‚
â””â”€ Citation discovery service (in routes/agents.py)

Connections: Services â†’ Orchestration
Latency: 5-10ms (business logic)
Parallelization: None at this level
Bottleneck: Sequential dataset enrichment
Optimization: 
  ðŸŽ¯ HIGH IMPACT:
    - Parallelize _build_dataset_responses() 
      (Currently enriches datasets sequentially!)
    - Use asyncio.gather() for batch enrichment


LEVEL 4 (Orchestration)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 3
â”œâ”€ orchestrator.py::SearchOrchestrator
â”‚   â”œâ”€ search() - Main coordinator
â”‚   â”œâ”€ _search_geo() - GEO search thread
â”‚   â”œâ”€ _search_publications_pubmed() - PubMed thread
â”‚   â””â”€ _search_publications_openalex() - OpenAlex thread
â”‚
â”œâ”€ geo_cache.py::GEOCache
â”‚   â”œâ”€ get() - Cache lookup
â”‚   â””â”€ _auto_discover_and_populate() - Discovery
â”‚
â””â”€ geo_discovery.py::GEOCitationDiscovery
    â””â”€ find_citing_papers() - Citation search

Connections: Orchestrators â†’ Cache/Database/Clients
Latency: 10-50ms (coordination overhead)
Parallelization: âœ… ACTIVE
  - SearchOrchestrator: 3 parallel threads
  - GEOCitationDiscovery: 4 parallel threads
Bottleneck: Waiting for slowest thread
Optimization:
  âœ… Already parallelized
  ðŸŽ¯ Consider:
    - Add timeout for slow threads
    - Return partial results if one fails
    - Load balancing across sources


LEVEL 5 (Cache & Storage)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 2
â”œâ”€ redis_cache.py::RedisCache
â”‚   â”œâ”€ get_search_result()
â”‚   â”œâ”€ set_search_result()
â”‚   â””â”€ get_geo_metadata()
â”‚
â””â”€ unified_db.py::UnifiedDatabase
    â”œâ”€ get_complete_geo_data()
    â”œâ”€ insert_geo_dataset()
    â”œâ”€ insert_universal_identifier()
    â””â”€ get_publications_by_geo()

Connections: Cache â†” Database â†” Orchestration
Latency:
  - Redis: 0.2-1ms
  - UnifiedDB: 50-200ms (depending on query complexity)
Parallelization: None (cache lookups are sequential)
Bottleneck: UnifiedDB JOIN queries
Optimization:
  ðŸŽ¯ MEDIUM IMPACT:
    - Add database indexes:
      CREATE INDEX idx_geo_id ON universal_identifiers(geo_id)
      CREATE INDEX idx_pmid ON universal_identifiers(pmid)
    - Use connection pooling
    - Consider PostgreSQL for production (faster than SQLite)
    - Denormalize frequently-accessed data


LEVEL 6 (Data Discovery)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 2
â”œâ”€ geo_client.py::GEOClient
â”‚   â”œâ”€ search()
â”‚   â”œâ”€ get_metadata()
â”‚   â””â”€ batch_get_metadata()
â”‚
â””â”€ geo_discovery.py::GEOCitationDiscovery
    â”œâ”€ _find_via_citation()
    â””â”€ _find_via_geo_mention()

Connections: Discovery â†’ API Clients
Latency: 1-5 seconds (aggregated)
Parallelization: âœ… ACTIVE (citation discovery uses threads)
Bottleneck: External API rate limits
Optimization:
  âœ… Already parallelized
  ðŸŽ¯ Consider:
    - Implement request batching
    - Add circuit breaker for failing sources
    - Cache negative results (404s)


LEVEL 7 (API Clients)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 5
â”œâ”€ ncbi_client.py::NCBIClient
â”œâ”€ openalex_client.py::OpenAlexClient
â”œâ”€ pubmed_client.py::PubMedClient
â”œâ”€ semantic_scholar_client.py::SemanticScholarClient
â””â”€ europepmc_client.py::EuropePMCClient

Connections: Clients â†’ External APIs
Latency: 500ms - 5 seconds per request
Parallelization: âœ… ACTIVE (multiple clients in parallel)
Bottleneck: 
  - Rate limits (NCBI: 3 req/s, S2: 1 req/s)
  - Network latency
  - API response time
Optimization:
  ðŸŽ¯ HIGH IMPACT:
    - Implement request queuing with priority
    - Add exponential backoff for 429 errors
    - Use HTTP/2 multiplexing
    - Batch requests where APIs support it
    - Add API health monitoring


LEVEL 8 (External APIs)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Nodes: 5
â”œâ”€ NCBI E-utilities (eutils.ncbi.nlm.nih.gov)
â”œâ”€ OpenAlex API (api.openalex.org)
â”œâ”€ PubMed API (same as NCBI)
â”œâ”€ Semantic Scholar API (api.semanticscholar.org)
â””â”€ Europe PMC API (www.ebi.ac.uk/europepmc)

Connections: HTTP â†’ External servers
Latency: 200ms - 10 seconds (variable)
Parallelization: N/A (external systems)
Bottleneck: External API performance
Optimization:
  âš ï¸ Out of our control
  ðŸŽ¯ Mitigation strategies:
    - Aggressive caching
    - Fallback to alternative sources
    - Request deduplication
    - Monitor API status
```

---

## Performance Analysis - Critical Bottlenecks

### Identified Bottlenecks (Sorted by Impact)

```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ #   â”‚ Bottleneck                   â”‚ Level   â”‚ Latency  â”‚ Impact      â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1   â”‚ Sequential Dataset Enrichmentâ”‚ L3      â”‚ N*50ms   â”‚ ðŸ”´ CRITICAL â”‚
â”‚     â”‚ (_build_dataset_responses)   â”‚         â”‚ N=50â†’2.5sâ”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2   â”‚ Citation Discovery (4 sourcesâ”‚ L6-L8   â”‚ 8-25s    â”‚ ðŸ”´ CRITICAL â”‚
â”‚     â”‚ in parallel, wait for slowestâ”‚         â”‚          â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3   â”‚ UnifiedDB JOIN Queries       â”‚ L5      â”‚ 50-200ms â”‚ ðŸŸ¡ HIGH     â”‚
â”‚     â”‚ (no indexes on geo_id, pmid) â”‚         â”‚          â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4   â”‚ External API Rate Limits     â”‚ L7-L8   â”‚ Variable â”‚ ðŸŸ¡ HIGH     â”‚
â”‚     â”‚ (NCBI: 3/s, S2: 1/s)         â”‚         â”‚          â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5   â”‚ Middleware Chain (CORS, Auth)â”‚ L2      â”‚ 3-5ms    â”‚ ðŸŸ¢ MEDIUM   â”‚
â”‚     â”‚ on every request             â”‚         â”‚          â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6   â”‚ GEO Metadata Fetch (NCBI)    â”‚ L6-L8   â”‚ 1-3s     â”‚ ðŸŸ¢ MEDIUM   â”‚
â”‚     â”‚ (sequential esummary calls)  â”‚         â”‚          â”‚             â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Optimization Recommendations

### ðŸ”´ CRITICAL PRIORITY

#### 1. Parallelize Dataset Enrichment (L3)
**Current Code:**
```python
# services/search_service.py::_build_dataset_responses()
datasets = []
for ranked in ranked_datasets:
    geo_data = await self.geo_cache.get(ranked.dataset.geo_id)  # SEQUENTIAL!
    # ... enrichment ...
    datasets.append(dataset_response)
```

**Optimized Code:**
```python
# NEW: Parallel enrichment
async def _build_dataset_responses(self, ranked_datasets):
    async def enrich_single(ranked):
        geo_data = await self.geo_cache.get(ranked.dataset.geo_id)
        # ... enrichment logic ...
        return dataset_response
    
    # Execute all enrichments in parallel
    dataset_responses = await asyncio.gather(*[
        enrich_single(ranked) for ranked in ranked_datasets
    ])
    
    return dataset_responses
```

**Impact:**
- Current: 50 datasets * 50ms = 2.5 seconds (sequential)
- Optimized: max(50ms) = 50ms (parallel)
- **Speedup: 50x for uncached datasets**

---

#### 2. Add Database Indexes (L5)
**Current Schema:**
```sql
-- No indexes on foreign keys!
CREATE TABLE universal_identifiers (
    geo_id TEXT,
    pmid TEXT,
    -- ...
);
```

**Optimized Schema:**
```sql
-- Add indexes for JOIN optimization
CREATE INDEX idx_universal_geo_id ON universal_identifiers(geo_id);
CREATE INDEX idx_universal_pmid ON universal_identifiers(pmid);
CREATE INDEX idx_geo_dataset_id ON geo_datasets(geo_id);

-- Composite index for common queries
CREATE INDEX idx_geo_pmid_composite ON universal_identifiers(geo_id, pmid);
```

**Impact:**
- Current: 50-200ms for get_complete_geo_data() with JOINs
- Optimized: 5-20ms (10-40x faster)
- **Critical for auto-discovery performance**

---

### ðŸŸ¡ HIGH PRIORITY

#### 3. Implement Timeout & Partial Results (L6)
**Current Code:**
```python
# geo_discovery.py - waits for ALL sources
results = []
for future in futures.as_completed(citation_futures):
    results.extend(future.result())  # Blocks until ALL complete
```

**Optimized Code:**
```python
# NEW: Timeout with partial results
results = []
timeout = 10  # seconds

try:
    for future in futures.as_completed(citation_futures, timeout=timeout):
        try:
            results.extend(future.result(timeout=1))
        except Exception as e:
            logger.warning(f"Source failed: {e}")
            continue  # Skip failed source
except concurrent.futures.TimeoutError:
    logger.warning(f"Citation discovery timeout after {timeout}s - returning partial results")

return results  # Return what we have so far
```

**Impact:**
- Current: Wait up to 25 seconds for slowest source
- Optimized: Return after 10 seconds with partial results
- **User sees results 2.5x faster**

---

#### 4. Batch GEO Metadata Fetching (L6)
**Current:**
```python
# geo_client.py::batch_get_metadata() - calls esummary ONCE per ID
for geo_id in geo_ids:
    metadata = await self._fetch_single_metadata(geo_id)  # N requests!
```

**Optimized:**
```python
# NEW: Batch esummary (NCBI supports up to 200 IDs)
async def batch_get_metadata(self, geo_ids):
    batches = [geo_ids[i:i+200] for i in range(0, len(geo_ids), 200)]
    
    all_metadata = {}
    for batch in batches:
        # Single request for 200 IDs
        response = await self.ncbi_client.esummary(
            db="gds", 
            ids=",".join(batch)  # Comma-separated
        )
        # Parse all at once
        all_metadata.update(self._parse_batch_response(response))
    
    return all_metadata
```

**Impact:**
- Current: 50 datasets = 50 HTTP requests = 50 * 1s = 50 seconds
- Optimized: 50 datasets = 1 HTTP request = 1.5 seconds
- **Speedup: 33x**

---

### ðŸŸ¢ MEDIUM PRIORITY

#### 5. Cache Negative Results (L7)
**Current:**
```python
# No caching for 404s - retries same failed requests
try:
    paper = await openalex_client.get_work(pmid)
except NotFoundError:
    return None  # No caching!
```

**Optimized:**
```python
# Cache 404s for 24 hours
try:
    paper = await openalex_client.get_work(pmid)
except NotFoundError:
    # Cache negative result
    await redis.setex(f"not_found:openalex:{pmid}", 86400, "1")
    return None
```

**Impact:**
- Prevents redundant API calls for missing papers
- Reduces external API load by ~20%

---

#### 6. Request Deduplication (L7)
**Current:**
```python
# Multiple threads may request same paper simultaneously
# Thread 1: openalex.get_work("12345")
# Thread 2: semantic_scholar.get_paper("12345")  # Duplicate!
```

**Optimized:**
```python
# Global request deduplication with asyncio locks
_pending_requests = {}
_locks = defaultdict(asyncio.Lock)

async def deduplicated_request(key, fetch_fn):
    async with _locks[key]:
        if key in _pending_requests:
            return await _pending_requests[key]
        
        task = asyncio.create_task(fetch_fn())
        _pending_requests[key] = task
        
        try:
            result = await task
            return result
        finally:
            del _pending_requests[key]
```

**Impact:**
- Eliminates duplicate requests during parallel discovery
- Reduces API calls by 15-30%

---

## Optimization Implementation Plan

### Phase 1: Quick Wins (1-2 days)
```
1. Add database indexes (30 min)
   â””â”€ Impact: 10-40x faster DB queries
   
2. Parallelize dataset enrichment (2 hours)
   â””â”€ Impact: 50x faster search results
   
3. Add timeout to citation discovery (1 hour)
   â””â”€ Impact: 2.5x faster for slow sources
```

### Phase 2: High-Value (3-5 days)
```
4. Batch GEO metadata fetching (4 hours)
   â””â”€ Impact: 33x fewer API calls
   
5. Cache negative results (2 hours)
   â””â”€ Impact: 20% reduction in API calls
   
6. Request deduplication (4 hours)
   â””â”€ Impact: 15-30% fewer duplicate requests
```

### Phase 3: Infrastructure (1-2 weeks)
```
7. Migrate to PostgreSQL (if needed)
   â””â”€ Impact: Better concurrency, faster JOINs
   
8. Implement connection pooling
   â””â”€ Impact: Lower latency, better scalability
   
9. Add API health monitoring
   â””â”€ Impact: Proactive failover, better reliability
```

---

## Expected Performance After Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario             â”‚ Current  â”‚ Optimizedâ”‚ Improvement â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cached search        â”‚ 100ms    â”‚ 50ms     â”‚ 2x faster   â”‚
â”‚ First search (50 GEOsâ”‚ 30s      â”‚ 5s       â”‚ 6x faster   â”‚
â”‚ Auto-discovery       â”‚ 25s      â”‚ 10s      â”‚ 2.5x faster â”‚
â”‚ DB query (JOIN)      â”‚ 200ms    â”‚ 20ms     â”‚ 10x faster  â”‚
â”‚ Enrichment (50 GEOs) â”‚ 2.5s     â”‚ 50ms     â”‚ 50x faster  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

**Tree Structure Benefits:**
- **DFS Analysis:** Identified critical path bottlenecks (14 depth levels)
- **BFS Analysis:** Found parallelization opportunities at each level
- **Optimization Targets:** 6 high-impact improvements identified

**Key Findings:**
1. **Sequential enrichment (L3)** is the #1 bottleneck â†’ Parallelize with asyncio.gather()
2. **Missing DB indexes (L5)** cause slow JOINs â†’ Add indexes on geo_id, pmid
3. **Citation discovery timeout (L6)** waits for slowest source â†’ Add 10s timeout
4. **Batch metadata fetching (L6)** makes N requests â†’ Use batch esummary (1 request)

**Expected Impact:**
- Overall search: **6x faster** (30s â†’ 5s)
- Dataset enrichment: **50x faster** (2.5s â†’ 50ms)
- Database queries: **10x faster** (200ms â†’ 20ms)

**Implementation Priority:**
1. ðŸ”´ Add DB indexes (30 min, 10x impact)
2. ðŸ”´ Parallelize enrichment (2 hours, 50x impact)
3. ðŸŸ¡ Batch GEO fetching (4 hours, 33x impact)
4. ðŸŸ¡ Add discovery timeout (1 hour, 2.5x impact)

