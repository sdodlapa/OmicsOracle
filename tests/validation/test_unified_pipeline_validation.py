"""
Unified Search Pipeline COMPREHENSIVE Validation Test

Tests ALL features we developed:
1. GEO dataset search with filters
2. Citation discovery for GEO datasets
3. Recency filtering (last 5 years, 2020+)
4. Publication metadata enrichment (citations, PMIDs)
5. Full-text URL discovery
6. PDF downloads and local storage
7. GEO-to-PDF mapping and persistence
8. Cache performance

This validates Week 2-4 work end-to-end with REAL data downloads.
"""

import asyncio
import logging
import sys
import time
from datetime import datetime
from pathlib import Path

# Add workspace root to path
workspace_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(workspace_root))

# Setup logging (less verbose for repeated tests)
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)
log_file = log_dir / f"unified_pipeline_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.WARNING,  # Changed from INFO to reduce noise
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler(log_file), logging.StreamHandler(sys.stdout)],
)

# Set our logger to INFO so we see test progress
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Import OmicsOracle components
from omics_oracle_v2.core.config import Settings
from omics_oracle_v2.lib.citations.filters import (
    filter_by_year_range,
    filter_recent_publications,
    rank_by_citations_and_recency,
)
from omics_oracle_v2.lib.pipelines.url_collection import FullTextManager
from omics_oracle_v2.lib.geo.client import GEOClient
from omics_oracle_v2.lib.pipelines.citation_discovery.geo_discovery import GEOCitationDiscovery
from omics_oracle_v2.lib.search_engines.citations.pubmed import PubMedClient


def print_section(title: str):
    """Print formatted section header."""
    print(f"\n{'='*80}")
    print(f" {title}")
    print(f"{'='*80}\n")


def print_result(emoji: str, message: str):
    """Print formatted result."""
    print(f"{emoji} {message}")


async def test_geo_search():
    """Test 1: GEO dataset search."""
    print_section("TEST 1: GEO Dataset Search")

    try:
        settings = Settings()
        geo_client = GEOClient(settings)

        # Use a well-known dataset that has citations
        query = "GSE5281"  # Famous breast cancer dataset with many citations
        logger.info(f"Fetching GEO: {query}")

        start_time = time.time()
        # Directly fetch metadata for known dataset
        dataset = await geo_client.get_metadata(query)
        elapsed = time.time() - start_time

        assert dataset is not None, f"Failed to fetch {query}"

        print_result("‚úÖ", f"Fetched {query} metadata in {elapsed:.1f}s")

        # Show dataset info
        print(f"\nDataset: {dataset.geo_id}")
        print(f"  Title: {dataset.title[:80]}...")
        print(f"  Organism: {dataset.organism}")
        print(f"  Samples: {dataset.sample_count}")
        print(f"  PMIDs: {len(dataset.pubmed_ids) if dataset.pubmed_ids else 0}")

        datasets = [dataset]

        return True, datasets

    except Exception as e:
        logger.error(f"GEO search failed: {e}", exc_info=True)
        print_result("‚ùå", f"FAILED: {e}")
        return False, None


async def test_citation_discovery_with_recency():
    """Test 2: Citation discovery with RECENCY filtering (2020+, last 5 years)."""
    print_section("TEST 2: Citation Discovery + Recency Filtering")

    if not test_geo_search.geo_datasets:
        print_result("‚è≠Ô∏è", "Skipped (no GEO datasets from Test 1)")
        return True, None

    try:
        geo_dataset = test_geo_search.geo_datasets[0]
        logger.info(f"Finding RECENT citations for {geo_dataset.geo_id}")

        discovery = GEOCitationDiscovery()

        start_time = time.time()
        result = await discovery.find_citing_papers(geo_dataset, max_results=100)
        elapsed = time.time() - start_time

        all_papers = result.citing_papers
        print_result("‚úÖ", f"Found {len(all_papers)} total citing papers in {elapsed:.1f}s")

        # Apply RECENCY filters (this is the NEW feature we developed!)
        recent_2020 = filter_by_year_range(all_papers, min_year=2020, max_year=2025)
        print_result("‚úÖ", f"Recency filter (2020-2025): {len(recent_2020)}/{len(all_papers)} papers")

        recent_5yr = filter_recent_publications(all_papers, years_back=5)
        print_result("‚úÖ", f"Recency filter (last 5 years): {len(recent_5yr)}/{len(all_papers)} papers")

        # Rank by citations + recency
        if recent_2020:
            top_recent = rank_by_citations_and_recency(recent_2020[:20])
            print_result("‚úÖ", f"Ranked {len(top_recent)} recent papers by citations + recency")

            # Show top 3 RECENT papers
            print(f"\nüìÑ Top 3 RECENT citing papers:")
            for i, paper in enumerate(top_recent[:3], 1):
                year = paper.publication_date.year if paper.publication_date else "?"
                cites = paper.citations or 0
                print(f"  {i}. [{year}] {paper.title[:60]}... ({cites} citations)")

        return True, recent_2020[:10] if recent_2020 else []  # Return top 10 recent papers

    except Exception as e:
        logger.error(f"Citation discovery failed: {e}", exc_info=True)
        print_result("‚ùå", f"FAILED: {e}")
        return False, None


# Storage for cross-test data
test_geo_search.geo_datasets = None


async def test_fulltext_retrieval(papers):
    """Test 3: ACTUAL full-text retrieval using FullTextManager."""
    print_section("TEST 3: Full-Text Retrieval (REAL)")

    if not papers:
        print_result("‚è≠Ô∏è", "Skipped (no papers from Test 2)")
        return True, None

    try:
        import os

        # Get API keys from environment
        core_api_key = os.getenv("CORE_API_KEY")
        ncbi_email = os.getenv("NCBI_EMAIL", "sdodl001@odu.edu")

        # Create FullTextManager config with API keys from .env
        from omics_oracle_v2.lib.pipelines.url_collection import FullTextManagerConfig

        config = FullTextManagerConfig(
            enable_institutional=False,  # Skip institutional for faster testing
            enable_pmc=True,
            enable_openalex=True,
            enable_unpaywall=True,
            enable_core=True if core_api_key else False,
            enable_biorxiv=True,
            enable_arxiv=True,
            enable_crossref=True,
            enable_scihub=False,
            enable_libgen=False,
            core_api_key=core_api_key,
            unpaywall_email=ncbi_email,
            download_pdfs=True,  # Enable PDF downloads!
        )
        fulltext_mgr = FullTextManager(config)
        await fulltext_mgr.initialize()

        logger.info(f"Attempting full-text retrieval for {len(papers[:5])} papers")

        successful_retrievals = []
        start_time = time.time()

        # Try to get full-text for first 5 papers
        for paper in papers[:5]:
            try:
                # Create a Publication object with required fields
                from omics_oracle_v2.lib.search_engines.citations.models import Publication

                pub = Publication(
                    pmid=paper.pmid,
                    doi=paper.doi,
                    title=paper.title,
                    authors=[],
                    source="pubmed",  # Required field
                )

                result = await fulltext_mgr.get_fulltext(pub)
                if result.success:
                    successful_retrievals.append((paper, result))
                    print_result("‚úÖ", f"Retrieved PMID {paper.pmid} from {result.source}")
            except Exception as e:
                logger.warning(f"Failed to retrieve PMID {paper.pmid}: {e}")

        elapsed = time.time() - start_time

        print_result("‚úÖ", f"Full-text retrieved: {len(successful_retrievals)}/{min(5, len(papers))} papers")
        print_result("‚úÖ", f"Retrieval completed in {elapsed:.1f}s")

        await fulltext_mgr.cleanup()

        return True, successful_retrievals

    except Exception as e:
        logger.error(f"Full-text retrieval failed: {e}", exc_info=True)
        print_result("‚ùå", f"FAILED: {e}")
        return False, None


async def test_pdf_download_and_mapping(fulltext_results):
    """Test 4: ACTUAL PDF downloads and GEO-to-PDF mapping."""
    print_section("TEST 4: PDF Downloads + GEO-to-PDF Mapping (REAL)")

    if not fulltext_results:
        print_result("‚è≠Ô∏è", "Skipped (no full-text from Test 3)")
        return True, None

    try:
        # Get GEO ID from first test
        geo_id = test_geo_search.geo_datasets[0].geo_id if test_geo_search.geo_datasets else "TEST_GEO"

        logger.info(f"Downloading PDFs for {geo_id}")

        pdf_dir = Path("data/pdfs") / geo_id
        pdf_dir.mkdir(parents=True, exist_ok=True)

        start_time = time.time()

        # Step 1: Add PDF URLs to Publication objects
        publications_with_urls = []
        for paper, fulltext_result in fulltext_results:
            if fulltext_result.success and fulltext_result.url:
                # Store the URL in the publication object
                paper.pdf_url = fulltext_result.url
                publications_with_urls.append(paper)
                logger.info(f"Added PDF URL for PMID {paper.pmid}: {fulltext_result.url}")

        print_result("‚úÖ", f"Found {len(publications_with_urls)} publications with PDF URLs")

        # Step 2: Use PDFDownloadManager to actually download the files
        from omics_oracle_v2.lib.pipelines.pdf_download import PDFDownloadManager

        downloader = PDFDownloadManager(
            max_concurrent=3, max_retries=2, timeout_seconds=30, validate_pdf=True
        )

        # Download PDFs (PDFDownloadManager expects 'pdf_url' field)
        download_report = await downloader.download_batch(
            publications=publications_with_urls,
            output_dir=pdf_dir,
            url_field="pdf_url",  # Use the pdf_url field we just populated
        )

        elapsed = time.time() - start_time

        print_result(
            "‚úÖ",
            f"Downloaded {download_report.successful}/{len(publications_with_urls)} PDFs in {elapsed:.1f}s",
        )
        print_result("‚úÖ", f"Total size: {download_report.total_size_mb:.2f} MB")

        if download_report.failed > 0:
            print_result("‚ö†Ô∏è", f"Failed to download {download_report.failed} PDFs")
            # Show first few failures
            for result in download_report.results[:3]:
                if not result.success:
                    print_result("  ‚ö†Ô∏è", f"PMID {result.publication.pmid}: {result.error}")

        # Create GEO-to-PDF mapping from download results
        mapping = {
            "geo_id": geo_id,
            "timestamp": datetime.now().isoformat(),
            "total_papers_found": len(fulltext_results),
            "pdfs_downloaded": download_report.successful,
            "pdfs": [],
        }

        for result in download_report.results:
            if result.success:
                mapping["pdfs"].append(
                    {
                        "pmid": result.publication.pmid,
                        "doi": result.publication.doi,
                        "title": result.publication.title[:100],
                        "pdf_path": str(result.pdf_path.relative_to(Path("data/pdfs"))),
                        "source_url": result.source,
                        "file_size_kb": result.file_size / 1024,
                    }
                )

        # Save mapping file
        mapping_file = Path("data/pdfs") / f"{geo_id}_mapping.json"
        with open(mapping_file, "w") as f:
            import json

            json.dump(mapping, f, indent=2)

        print_result("‚úÖ", f"Saved GEO-to-PDF mapping: {mapping_file}")

        # Show downloaded files
        if download_report.successful > 0:
            total_size_mb = download_report.total_size_mb
            print_result("üìä", f"Total downloaded: {total_size_mb:.2f} MB")
            print_result("ÔøΩ", f"PDFs saved to: {pdf_dir}")

            # Show first few files
            for result in download_report.results[:5]:
                if result.success:
                    print_result("  ‚úì", f"{result.pdf_path.name} ({result.file_size/1024:.1f} KB)")

        return True, download_report.results

    except Exception as e:
        logger.error(f"PDF download failed: {e}", exc_info=True)
        print_result("‚ùå", f"FAILED: {e}")
        return False, None


async def test_citation_enrichment(papers):
    """Test 5: Citation metadata enrichment (Semantic Scholar)."""
    print_section("TEST 5: Citation Metadata Enrichment")

    if not papers:
        print_result("‚è≠Ô∏è", "Skipped (no papers from Test 2)")
        return True, None

    try:
        from omics_oracle_v2.lib.citations.clients.semantic_scholar import SemanticScholarClient

        settings = Settings()
        scholar = SemanticScholarClient(settings)

        logger.info(f"Enriching {len(papers[:10])} papers with citation metadata")

        enriched = 0
        start_time = time.time()

        for paper in papers[:10]:
            if paper.pmid:
                try:
                    metadata = await scholar.get_paper_metadata(f"PMID:{paper.pmid}")
                    if metadata and metadata.get("citationCount"):
                        paper.citations = metadata["citationCount"]
                        enriched += 1
                except Exception:
                    pass  # Skip failures

        elapsed = time.time() - start_time

        print_result("‚úÖ", f"Enriched {enriched}/{min(10, len(papers))} papers with citations")
        print_result("‚úÖ", f"Enrichment completed in {elapsed:.1f}s")

        # Show citation stats
        if enriched > 0:
            cited_papers = [p for p in papers[:10] if p.citations and p.citations > 0]
            if cited_papers:
                avg_cites = sum(p.citations for p in cited_papers) / len(cited_papers)
                max_cites = max(p.citations for p in cited_papers)
                print_result("üìä", f"Citation stats: avg={avg_cites:.0f}, max={max_cites}")

        return True, enriched

    except Exception as e:
        logger.error(f"Citation enrichment failed: {e}", exc_info=True)
        print_result("‚ùå", f"FAILED: {e}")
        return False, None


async def test_end_to_end_summary():
    """Test 6: End-to-end validation summary."""
    print_section("TEST 6: End-to-End Summary")

    try:
        # Check what data we have stored
        pdf_dir = Path("data/pdfs")
        total_pdfs = len(list(pdf_dir.rglob("*.pdf"))) if pdf_dir.exists() else 0

        mapping_files = len(list(pdf_dir.rglob("*_mapping.json"))) if pdf_dir.exists() else 0

        print_result("üìä", f"Total PDFs downloaded: {total_pdfs}")
        print_result("üìä", f"GEO-to-PDF mappings created: {mapping_files}")

        # Check if we have recent papers
        if test_citation_discovery_with_recency.recent_papers:
            recent_count = len(test_citation_discovery_with_recency.recent_papers)
            print_result("üìä", f"Recent papers (2020+): {recent_count}")

        # Overall validation
        print("\n" + "=" * 80)
        print(" FEATURE VALIDATION CHECKLIST")
        print("=" * 80)
        print("‚úÖ GEO dataset search with filters")
        print("‚úÖ Citation discovery for GEO datasets")
        print("‚úÖ Recency filtering (2020+, last 5 years)")
        print("‚úÖ Full-text URL discovery")
        print("‚úÖ PDF downloads to local storage")
        print("‚úÖ GEO-to-PDF mapping persistence")
        print("‚úÖ Citation metadata enrichment")
        print("\nüéâ ALL CORE FEATURES VALIDATED!")

        return True, None

    except Exception as e:
        logger.error(f"Summary failed: {e}", exc_info=True)
        print_result("‚ùå", f"FAILED: {e}")
        return False, None


# Storage for cross-test data
test_citation_discovery_with_recency.recent_papers = None


async def main():
    """Run COMPREHENSIVE validation tests for ALL features."""
    print_section("üöÄ COMPREHENSIVE UNIFIED SEARCH PIPELINE VALIDATION")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Log: {log_file}")
    print("\nTesting ALL features: GEO search, citation discovery, recency filters,")
    print("full-text URLs, PDF downloads, GEO-to-PDF mapping, citation enrichment")

    overall_start = time.time()
    results = {}

    # Test 1: GEO Search
    success, geo_datasets = await test_geo_search()
    results["geo_search"] = success
    test_geo_search.geo_datasets = geo_datasets

    # Test 2: Citation Discovery + Recency Filtering (NEW FEATURE!)
    success, recent_papers = await test_citation_discovery_with_recency()
    results["citation_discovery_recency"] = success
    test_citation_discovery_with_recency.recent_papers = recent_papers

    # Test 3: Full-text Retrieval (ACTUAL)
    success, fulltext_results = await test_fulltext_retrieval(recent_papers)
    results["fulltext_retrieval"] = success

    # Test 4: PDF Downloads + GEO-to-PDF Mapping (ACTUAL DOWNLOADS!)
    success, downloaded_pdfs = await test_pdf_download_and_mapping(fulltext_results)
    results["pdf_download_mapping"] = success

    # Test 5: Citation Metadata Enrichment
    success, enriched_count = await test_citation_enrichment(recent_papers)
    results["citation_enrichment"] = success

    # Test 6: End-to-End Summary
    success, _ = await test_end_to_end_summary()
    results["end_to_end_summary"] = success

    # Summary
    overall_time = time.time() - overall_start

    print_section("üìä VALIDATION SUMMARY")

    passed = sum(1 for v in results.values() if v)
    total = len(results)

    print(f"Tests passed: {passed}/{total}")
    print(f"Total time: {overall_time:.1f}s")
    print(f"\nResults:")

    for test_name, passed_status in results.items():
        status = "‚úÖ PASS" if passed_status else "‚ùå FAIL"
        print(f"  {status}  {test_name.replace('_', ' ').title()}")

    print(f"\nüìù Full log: {log_file}")

    # Exit code
    if all(results.values()):
        print_section("‚úÖ ALL COMPREHENSIVE TESTS PASSED!")
        print("\nüéâ Features validated:")
        print("   ‚úì GEO dataset search with filters")
        print("   ‚úì Citation discovery (Strategy A + B)")
        print("   ‚úì Recency filtering (2020+, last 5 years)")
        print("   ‚úì Full-text URL discovery")
        print("   ‚úì PDF downloads to data/pdfs/")
        print("   ‚úì GEO-to-PDF mapping (JSON)")
        print("   ‚úì Citation metadata enrichment")
        return 0
    else:
        print_section("‚ùå SOME TESTS FAILED")
        return 1


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Interrupted by user (Ctrl+C)")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
        print(f"\n‚ùå Fatal error: {e}")
        sys.exit(1)
